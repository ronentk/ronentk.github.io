<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ronen Tamari</title>
    <link>https://ronentk.github.io/</link>
      <atom:link href="https://ronentk.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Ronen Tamari</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 17 Nov 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://ronentk.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Ronen Tamari</title>
      <link>https://ronentk.github.io/</link>
    </image>
    
    <item>
      <title>Reframing our relation to our data</title>
      <link>https://ronentk.github.io/post/stig_data/</link>
      <pubDate>Fri, 17 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/post/stig_data/</guid>
      <description>&lt;p&gt;(&lt;a href=&#34;https://twitter.com/rtk254/status/1725560129527980305&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Originally posted on Twitter&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;We need new frames for rethinking our relation to data! In existing frames, data is inert &amp;amp; detached, remains we leave behind that are meaningless to us (data as &amp;ldquo;exhaust&amp;rdquo; or &amp;ldquo;oil&amp;rdquo;) but can be captured by corporations and converted to gold.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Stigmergy%20vs%20data%20exhaust%20blog%20post%20ee2f0f4e22434a8c84cc628598ea4c4d/Untitled.png&#34; alt=&#34;Untitled&#34;&gt;&lt;/p&gt;
&lt;p&gt;The importance of finding better frames cannot be overstated. For an especially deep (and disturbing) account of the costs of existing frames to our societies, see &lt;a href=&#34;https://colonizedbydata.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://colonizedbydata.com/&lt;/a&gt; by Nick Couldry and Ulises Mejías. Reframe, or be colonized.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Stigmergy%20vs%20data%20exhaust%20blog%20post%20ee2f0f4e22434a8c84cc628598ea4c4d/Untitled%201.png&#34; alt=&#34;Untitled&#34;&gt;&lt;/p&gt;
&lt;p&gt;In my opinion, a particularly powerful &amp;amp; expansive new frame for rethinking our relation to data is provided by an esoteric concept called &lt;em&gt;stigmergy&lt;/em&gt; (so underrated - if one could buy stock in words, this would be on the top of my list). Originally developed by insect researchers, stigmergy basically means that the trails we leave in our environment are essential drivers of individual and collective intelligence:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Stigmergy%20vs%20data%20exhaust%20blog%20post%20ee2f0f4e22434a8c84cc628598ea4c4d/Untitled%202.png&#34; alt=&#34;Untitled&#34;&gt;&lt;/p&gt;
&lt;p&gt;(&lt;a href=&#34;https://twitter.com/rtk254/status/1528742173289635840&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Stigmergy applies to humans - it’s not just for ants! Realizing how dependent we humans are on each other’s trails for navigating information invites a complete rethinking of our relation to our data. Data as our digital &lt;em&gt;trails&lt;/em&gt;, not digital exhaust!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Stigmergy%20vs%20data%20exhaust%20blog%20post%20ee2f0f4e22434a8c84cc628598ea4c4d/Untitled%203.png&#34; alt=&#34;Untitled&#34;&gt;&lt;/p&gt;
&lt;p&gt;Data trails are quite literally the extension of our individual and collective cognition - how we come to form beliefs and act intelligently in the world:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Stigmergy%20vs%20data%20exhaust%20blog%20post%20ee2f0f4e22434a8c84cc628598ea4c4d/Untitled%204.png&#34; alt=&#34;Untitled&#34;&gt;&lt;/p&gt;
&lt;p&gt;Stigmergy highlights the precarity of our current predicament, &lt;a href=&#34;https://twitter.com/rtk254/status/1707376204108468435&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the brokenness of our collective brain&lt;/a&gt;. But it also invites questions from which we can regenerate healthier information environments:&lt;/p&gt;
&lt;p&gt;What trails are we leaving, and for whom? What tools are we using to make trails and to discover them. What is the medium/ground (platform?) in which we’re leaving them? Who is profiting from the trails? How will other humans and machines find the trails, and how will they interpret them? How are our trails contributing to the collective?&lt;/p&gt;
&lt;p&gt;The answers to these questions are far from straightforward. Merely providing individuals with more control over their data is not enough, as discussed in the excellent &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3850456&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Artificial Intelligence and the Purpose of Social Systems&lt;/a&gt; by Sebastian Benthall Jake Goldenfein:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Stigmergy%20vs%20data%20exhaust%20blog%20post%20ee2f0f4e22434a8c84cc628598ea4c4d/Untitled%205.png&#34; alt=&#34;Untitled&#34;&gt;&lt;/p&gt;
&lt;p&gt;Rather, whole new &amp;ldquo;&lt;a href=&#34;https://twitter.com/gordonbrander/status/1201927566417784832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;knowledge ecologies&lt;/a&gt;&amp;rdquo; will have to be built, where &lt;a href=&#34;https://twitter.com/rtk254/status/1528742249915375616&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;data sovereignty is coupled with distributed collective governance, and w/ a plural + interoperable array of tools for leveraging stigmergic trails&lt;/a&gt; for prosocial content discovery, human-human connection, and all the other things we need to do to save our world.&lt;/p&gt;
&lt;p&gt;Shout out to the &lt;a href=&#34;https://twitter.com/rtk254/status/1725560226458263608&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Social Sensemaking Scenius&lt;/a&gt; where we&amp;rsquo;re working and playing with these ideas!🌱✨&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why You&#39;re an Ant in the Attention Economy</title>
      <link>https://ronentk.github.io/talk/john_ash_podcast_261023/</link>
      <pubDate>Sun, 29 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/talk/john_ash_podcast_261023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DeSci Sensemaking Networks</title>
      <link>https://ronentk.github.io/talk/desci_boston_23/</link>
      <pubDate>Sun, 22 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/talk/desci_boston_23/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sensemaking and Science Publishing</title>
      <link>https://ronentk.github.io/post/sci-sensemaking/</link>
      <pubDate>Sun, 27 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/post/sci-sensemaking/</guid>
      <description>&lt;p&gt;(&lt;a href=&#34;https://twitter.com/rtk254/status/1695785300360847415&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Originally posted on Twitter&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;A lot has been written about how the traditional science model of publishing and peer review is broken. One connection I haven’t seen made is with the &lt;strong&gt;sensemaking&lt;/strong&gt; literature, which studies how people understand complex topics.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Webpage%20post%20727888c09ba8472fae72c87c1f9efd4d/Untitled.png&#34; alt=&#34;Untitled&#34;&gt;&lt;/p&gt;
&lt;p&gt;Sensemaking is a powerful framework that has been used mainly for analyzing organizations and surprisingly less for scientific research. What is science if not a collective process for understanding our complex reality? Anyway, sensemaking really helps to see why the current traditional science model is broken, and also how we can improve things.&lt;/p&gt;
&lt;p&gt;While not directly mentioned in the figure below, classical publishing and peer review are at the end stages of a nested, iterative multi-scale feedback loop. Information search, filtering, relating concepts, etc are all part of the larger sensemaking process.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Webpage%20post%20727888c09ba8472fae72c87c1f9efd4d/Untitled%201.png&#34; alt=&#34;Untitled&#34;&gt;&lt;/p&gt;
&lt;p&gt;The sensemaking model shows us 2 important missing elements from trad-sci.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Publishing happens across diverse scales and formats, as demonstrated by the rapid rise of various &lt;a href=&#34;https://www.micropublication.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;micro&lt;/a&gt;/&lt;a href=&#34;https://nanopub.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nanopublishing&lt;/a&gt;/&lt;a href=&#34;https://oasis-lab.gitbook.io/roamresearch-discourse-graph-extension/fundamentals/what-is-a-discourse-graph&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;discourse graph&lt;/a&gt; initiatives&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;Webpage%20post%20727888c09ba8472fae72c87c1f9efd4d/Untitled%202.png&#34; alt=&#34;Untitled&#34;&gt;&lt;/p&gt;
&lt;p&gt;For example, nanopublishing aims at the &amp;ldquo;smallest units of publishable information&amp;rdquo;; even someone declaring they’ve read a paper can be a nanopub!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Webpage%20post%20727888c09ba8472fae72c87c1f9efd4d/Untitled%203.png&#34; alt=&#34;Untitled&#34;&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Feedback also happens across diverse scales and formats. This is why scientists spend so much time on social media- we don&amp;rsquo;t want to wait months for noisy peer reviews. Scientists posting on Twitter are actually nanopublishing but don’t know it yet!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;Webpage%20post%20727888c09ba8472fae72c87c1f9efd4d/Untitled%204.png&#34; alt=&#34;Untitled&#34;&gt;&lt;/p&gt;
&lt;p&gt;That is a current blind spot in our scientific infrastructure! Social media networks are not open or FAIR, so are ill suited to host these “proto” nanopubs.&lt;/p&gt;
&lt;p&gt;The sensemaking framework suggests that for micro/nanopub efforts to succeed, they’ll need to be embedded in effective social feedback networks.&lt;/p&gt;
&lt;p&gt;The idea of social networks for scientists has &lt;a href=&#34;https://twitter.com/BrettButtliere/status/1609612797154828288&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;been around for a long time&lt;/a&gt;, &lt;a href=&#34;https://www.latimes.com/business/story/2023-08-25/column-scientists-used-to-love-twitter-thanks-to-elon-musk-theyre-giving-up-on-it&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;but with social media going FUBAR&lt;/a&gt;, IMO it’s high time to revisit these ideas and integrate them with the exciting progress on micro/nanopublishing&lt;/p&gt;
&lt;p&gt;I recently presented these ideas at MetaScience 2023  - check out &lt;a href=&#34;https://ronentk.github.io/publication/metasci-sensemaking-23/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ronentk.github.io/publication/metasci-sensemaking-23/&lt;/a&gt; for paper/slides/etc! Still work in progress, and we&amp;rsquo;d love to engage with fellow sensemakers interested in this space!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open Science Sensemaking: Adding Sensemaking Data to the Scientific Record for Making Sense of Science</title>
      <link>https://ronentk.github.io/talk/nanopubs_110723/</link>
      <pubDate>Tue, 11 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/talk/nanopubs_110723/</guid>
      <description></description>
    </item>
    
    <item>
      <title>From Users to (Sense)Makers: Open sourcing and interconnecting our collective sensemaking for healthier information ecologies</title>
      <link>https://ronentk.github.io/talk/dweb_camp_2023/</link>
      <pubDate>Fri, 23 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/talk/dweb_camp_2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>LLMs as hostile epistemic environments</title>
      <link>https://ronentk.github.io/post/llms-hostile-epistem-env/</link>
      <pubDate>Sun, 11 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/post/llms-hostile-epistem-env/</guid>
      <description>&lt;p&gt;Originally posted to Twitter at &lt;a href=&#34;https://twitter.com/rtk254/status/1667868407486619652&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://twitter.com/rtk254/status/1667868407486619652&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Blog%20version%20ba4efd5119884102979f9b725fb4376e/Untitled.png&#34; alt=&#34;Untitled&#34;&gt;&lt;/p&gt;
&lt;p&gt;There is a follow up to &lt;a href=&#34;https://philpapers.org/archive/NGUHEL.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this (excellent!) piece&lt;/a&gt; waiting to be written about LLMs as “hostile epistemic environments”, or “epistemic junk food”. I don’t have time for that yet, so meanwhile just a few thoughts 🧵:&lt;/p&gt;
&lt;p&gt;Hostile epistemic envs exploit our cognitive vulnerabilities and weaknesses, which are plentiful in an age where information overload is stretching cognition beyond its limits.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Blog%20version%20ba4efd5119884102979f9b725fb4376e/Untitled%201.png&#34; alt=&#34;Untitled&#34;&gt;&lt;/p&gt;
&lt;p&gt;I have a hunch that LLMs, like conspiracy theories, work becasue they offer an intense dose of clarity/coherence, particularly valuable in an overwhelmingly complex world.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Blog%20version%20ba4efd5119884102979f9b725fb4376e/Untitled%202.png&#34; alt=&#34;Untitled&#34;&gt;&lt;/p&gt;
&lt;p&gt;Hostile epistemic envs share important similarities with hostile nutritional envs (junk food): the companies creating them may not be intentionally trying to harm us, they want $$ and harm is a side effect.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Blog%20version%20ba4efd5119884102979f9b725fb4376e/Untitled%203.png&#34; alt=&#34;Untitled&#34;&gt;&lt;/p&gt;
&lt;p&gt;Creators of epistemic junk food will ofc say- “but we’re helping people”! A good heuristic might be - helping in the short or long term? Lonliness chatbots may alleviate loneliness like cigarettes relieve stress, but have negative side effects (for which corporations aren’t accountable) and shouldn’t be substitutes for more systemic approaches.&lt;/p&gt;
&lt;p&gt;This isn’t to say that there aren’t any non-“junk-food” uses of LLMs - there are many useful applications of the technology. But the “fairtrade/sustainable” uses are still rare comparatively to the junk uses, and those less exploitative uses are harder to turn a profit with.&lt;/p&gt;
&lt;p&gt;Despite their not-quite-intentional hostility, greed can make you do pretty awful stuff: today we shake our heads in disbelief that we were duped into thinking 7up for babies was not only ok, it was even wholesome. I hope that a few decades from now we’ll look back similarly at junk LLM products.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Blog%20version%20ba4efd5119884102979f9b725fb4376e/Untitled%204.png&#34; alt=&#34;Untitled&#34;&gt;&lt;/p&gt;
&lt;p&gt;However, with &lt;a href=&#34;https://www.sup.org/books/title/?id=28816&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;capitalism reaching deeper and deeper into our psyche in search of new territory to exploit&lt;/a&gt;, what will be left of us in a few decades is anyone’s guess 🤷&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Making sense of science: open access science needs open access to scholarly sensemaking data</title>
      <link>https://ronentk.github.io/publication/metasci-sensemaking-23/</link>
      <pubDate>Thu, 16 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/publication/metasci-sensemaking-23/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Breakpoint Transformers for Modeling and Tracking Intermediate Beliefs</title>
      <link>https://ronentk.github.io/publication/breakpoint-2022/</link>
      <pubDate>Thu, 06 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/publication/breakpoint-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Active Inference GuestStream: From Users to (Sense)Makers</title>
      <link>https://ronentk.github.io/talk/actinf_gueststream_0822/</link>
      <pubDate>Fri, 12 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/talk/actinf_gueststream_0822/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Communicating understanding of physical dynamics in natural language</title>
      <link>https://ronentk.github.io/publication/cliphy-2022/</link>
      <pubDate>Wed, 15 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/publication/cliphy-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>From Users to (Sense)Makers: On the Pivotal Role of Stigmergic Social Annotation in the Quest for Collective Sensemaking</title>
      <link>https://ronentk.github.io/publication/users-to-makers-2022/</link>
      <pubDate>Sun, 15 May 2022 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/publication/users-to-makers-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dyna-bAbI: unlocking bAbI&#39;s potential with dynamic synthetic benchmarking</title>
      <link>https://ronentk.github.io/publication/dyna-babi-2021/</link>
      <pubDate>Sun, 01 May 2022 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/publication/dyna-babi-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Scaling Creative Inspiration with Fine-Grained Functional Aspects of Ideas</title>
      <link>https://ronentk.github.io/publication/scaling-creation-2021/</link>
      <pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/publication/scaling-creation-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>&#39;Dark Research&#39; talk at Hebrew University NLP Seminar</title>
      <link>https://ronentk.github.io/talk/huji_seminar_0521/</link>
      <pubDate>Wed, 05 May 2021 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/talk/huji_seminar_0521/</guid>
      <description></description>
    </item>
    
    <item>
      <title>🔮Dark Research</title>
      <link>https://ronentk.github.io/post/dark_research/</link>
      <pubDate>Wed, 05 May 2021 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/post/dark_research/</guid>
      <description>&lt;p&gt;&amp;ldquo;Dark research&amp;rdquo; refers to skills and knowledge which are often significant factors in successful research, yet are rarely covered by any formal graduate university curricula. For example, (1) how to find and formulate research questions, (2) how to manage references &amp;amp; project workflows and not drown in the oceans of information, (3) how to write and maintain reproducible research code and experiments, and (4) how to forge connections and collaborations with the wider scientific community. Here are some &lt;a href=&#34;https://ronentk.github.io/talk/huji_seminar_0521/dark_research_slides_0521.pdf&#34;&gt;slides&lt;/a&gt; from a recent university seminar talk, presenting some of my own experience and tools for grappling with these challenges, centering around Notion for (2) (and pretty much everything else also 🙃), Weights and Biases for (3) and Twitter + Hugo Academic web pages for (4).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Process-Level Representation of Scientific Protocols with Interactive Annotation</title>
      <link>https://ronentk.github.io/publication/tamari-2020-process-eacl/</link>
      <pubDate>Fri, 16 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/publication/tamari-2020-process-eacl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cognitive Tools Lab, University of California, San Diego</title>
      <link>https://ronentk.github.io/talk/ucsd_0321/</link>
      <pubDate>Wed, 17 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/talk/ucsd_0321/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Seminar Talk at AI2 - Aristo</title>
      <link>https://ronentk.github.io/talk/aristo_0121/</link>
      <pubDate>Tue, 12 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/talk/aristo_0121/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Contributed Talk (WordPlay @ NeurIPS2020)</title>
      <link>https://ronentk.github.io/talk/wordplay_2020/</link>
      <pubDate>Wed, 09 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/talk/wordplay_2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>🔍 Managing Research in Notion!</title>
      <link>https://ronentk.github.io/post/notion/</link>
      <pubDate>Sun, 06 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/post/notion/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.notion.so/ronhome/Managing-Research-in-Notion-9cb021e748884b2f8936ffb98cca58a3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;External link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Process-Level Representation of Scientific Protocols with a Text-Based Game Annotation Interface</title>
      <link>https://ronentk.github.io/publication/tamari-2020-process-wp/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/publication/tamari-2020-process-wp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Poster at NAISys Virtual Conference</title>
      <link>https://ronentk.github.io/talk/naisys_1120/</link>
      <pubDate>Mon, 09 Nov 2020 13:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/talk/naisys_1120/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Distribution Based Compositionality Assessment (DBCA)</title>
      <link>https://ronentk.github.io/post/dbca/</link>
      <pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/post/dbca/</guid>
      <description>&lt;h1 id=&#34;background-compositional-generalizations&#34;&gt;Background: Compositional Generalizations&lt;/h1&gt;
&lt;p&gt;Compositional generalization, or models&#39; lack of it, is a hot topic in NLP right now. Compositional generalization (&lt;a href=&#34;http://ruccs.rutgers.edu/images/personal-zenon-pylyshyn/proseminars/Proseminar13/ConnectionistArchitecture.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fodor &amp;amp; Pylysyn, 1988&lt;/a&gt;) refers to the ability to make &amp;ldquo;infinite use of finite means&amp;rdquo; and is a cornerstone of human intelligence in general, and in particular language use: humans can combine a finite set of discrete elements (such as words) in limitless ways to create new meanings. An oft-quoted &lt;a href=&#34;https://arxiv.org/abs/1711.00350&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;example&lt;/a&gt; is that once humans learn the meaning of a new verb like &amp;ldquo;dax&amp;rdquo;, we effortlessly generalize to new novel combinations with known words, like &amp;ldquo;dax twice&amp;rdquo; or &amp;ldquo;dax slowly&amp;rdquo;. While crucial for more robust and efficient NLP, compositional generalization remains extremely challenging for state-of-the-art (SOTA) models, with a &lt;a href=&#34;http://arxiv.org/abs/2007.08970&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recent paper&lt;/a&gt; noting that despite the huge investment,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;General ML architecture improvements yield incremental, if limited, improvements in compositional generalization settings.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;How to even measure the ability for compositional generalization? Like so many concepts in natural language, intuitive explanations of compositional generalization are easy, but more precise definitions can be notoriously elusive. Many works (e.g., &lt;a href=&#34;https://arxiv.org/abs/1711.00350&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SCAN&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2003.05161&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gSCAN&lt;/a&gt;) adopt a simple approach of holding out some set of test samples which differ systematically from those seen in training. For example, showing a model what it means to &amp;ldquo;walk while spinning&amp;rdquo; and &amp;ldquo;push the circle&amp;rdquo; at train time, and then testing it on &amp;ldquo;push the circle while spinning&amp;rdquo;. While such approaches indeed test for compositional generalization, a limitation is that the holding out classes of samples is somewhat heuristic and hand-engineered, and doesn&amp;rsquo;t yield a precise, numeric or canonical quantification of train-test differences.&lt;/p&gt;
&lt;p&gt;Fortunately, a cool &lt;a href=&#34;https://openreview.net/forum?id=SygcCnNKwr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; presented this year at ICLR2020 makes some nice progress on this question. The paper, out of Google Brain, is called &lt;em&gt;&amp;ldquo;Measuring Compositional Generalization: A Comprehensive Method on Realistic Data&amp;rdquo;&lt;/em&gt;. They propose a method, Distribution Based Compositionality Assessment (DBCA), which allows constructing and more precisely quantifying the compositional-generalization gap between train and test splits.&lt;/p&gt;
&lt;p&gt;I really liked the paper and indeed found it comprehensive (including a 20+ page appendix), so this is my attempt at a walk-through and accompaniment to my (un-offical) &lt;a href=&#34;https://github.com/ronentk/dbca-splitter&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;re-implementation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Turn the dial on the train-test compositionality gap, and watch model performance drop! If it doesn&amp;rsquo;t: either you have a breakthrough, or I have a bug 🙃.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;media/dbca.gif&#34; alt=&#34;media/dbca.gif&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;dbca-in-a-nutshell&#34;&gt;DBCA in a nutshell&lt;/h1&gt;
&lt;p&gt;DBCA provides a method of systematically generating datasets with train and test splits diverging in a controllable and measurable way. The paper applies this method to generate a compositional question answering dataset called CFQ (Compositional Freebase Questions). They show that turning the divergence dial strongly corresponds with a decrease in model accuracy, while remaining compositional and human interpretable; A human who could correctly answer the training set examples of “Who directed Inception?” and “Did Christopher Nolan produce Goldfinger?” would have no trouble testing on questions such as “Did Christopher Nolan direct Goldfinger?” and &amp;ldquo;Who produced Inception?&amp;rdquo;, but current SOTA models fail dramatically in such settings.&lt;/p&gt;
&lt;p&gt;As the name implies, the magic happens by carefully controlling two distribution measures between train and test splits:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Atom Divergence: we&amp;rsquo;ll get formal later, but for now we can intuitively think of atoms as the elementary building blocks comprising each sample:  predicates liked &amp;ldquo;directed&amp;rdquo;, or entities like &amp;ldquo;Inception&amp;rdquo;. We want the atoms to be distributed roughly the same across train and test distributions→low atom divergence, as shown in Fig. 6 of the paper&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;media/fig6a.png&#34; alt=&#34;media/fig6a.png&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compound Divergence: Compounds represent different ways of composing atoms. “Did Christopher Nolan direct Goldfinger?” and &amp;ldquo;Who produced Inception?&amp;rdquo; are novel compounds over the same atoms from the training set (&amp;ldquo;directed&amp;rdquo;, &amp;ldquo;Inception&amp;rdquo;, etc). To measure compositionality, we want to create test sets with novel compounds not seen at train time:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;media/fig6c.png&#34; alt=&#34;media/fig6c.png&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;definitions&#34;&gt;Definitions&lt;/h1&gt;
&lt;h2 id=&#34;sample-definition&#34;&gt;Sample Definition&lt;/h2&gt;
&lt;p&gt;To meaningfully measure how samples differ from each other, let&amp;rsquo;s first understand how a sample is defined and generated in DBCA. Each sample is defined to be a directed a-cyclic graph (DAG), where nodes (or atoms) correspond to rule applications. Edges correspond to dependencies between rule applications.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nodes:&lt;/strong&gt; In CFQ, the paper mentions 4 kinds of rules used in question generation: grammar (211), inference (17), resolution (21) and knowledge (194). This yields a total of $N_{A}=443$ types of nodes, as can be seen in the histogram in Fig. 6 (above). Feel free to check out the paper for specific rule examples: this post is less about the particulars of CFQ and more the general workings of DBCA.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Edges:&lt;/strong&gt; DAG edges represent dependencies among the rules (nodes), an edge $A \to B$ means that rule $B$ strictly depends on rule $A$ in the sense that the generator cannot apply rule $B$ before applying rule $A$.&lt;/p&gt;
&lt;p&gt;Comparing graphs is known to be hard (graph isomorphism problem), therefore the DAG is normalized to ensure that a certain rule combination is represented using the same DAG across all the examples where it occurs. This is important for meaningfully comparing measures such as divergence.&lt;/p&gt;
&lt;h2 id=&#34;distributions-and-divergences&#34;&gt;Distributions and Divergences&lt;/h2&gt;
&lt;h3 id=&#34;atom-distribution&#34;&gt;Atom Distribution&lt;/h3&gt;
&lt;p&gt;For a sample set $T$, the &lt;em&gt;atom distribution,&lt;/em&gt;  $\mathcal{F}_{A}\left(T\right)$, is defined to be the frequency distribution of atoms in the set.&lt;/p&gt;
&lt;p&gt;Here $T$ is a collection of DAGs, and atoms correspond to graph node types. To be more concrete, you can imagine $\mathcal{F}_{A} \left( T \right)$ being represented by a $N_{A}$ dimensional vector containing the normalized frequency counts of each atom (rule application) across $T$.&lt;/p&gt;
&lt;p&gt;As mentioned, DBCA creates training and test sets which share &lt;strong&gt;similar&lt;/strong&gt; atom distributions.&lt;/p&gt;
&lt;h3 id=&#34;compound-distribution&#34;&gt;Compound Distribution&lt;/h3&gt;
&lt;p&gt;For a sample set $T$, the &lt;em&gt;compound distribution,&lt;/em&gt; $\mathcal{F}_{C}\left(T\right)$, is defined to be the frequency distribution of compounds in the set.&lt;/p&gt;
&lt;p&gt;In contrast to the atom distribution, we want the compound distribution to be divergent across train and test sets, in order to test models&#39; ability to recombine known atoms in novel ways. To represent $\mathcal{F}_{C}\left(T\right)$, you can similarly imagine a long vector containing the frequency counts of each &lt;em&gt;compound&lt;/em&gt; in $T$. Here we run into a problem though, due to combinatorial complexity considerations- the number of all possible compounds (sub-graphs) is generally exponential in the size of $T$.&lt;/p&gt;
&lt;p&gt;Therefore, for practical purposes, the compound distribution requires a more involved definition. Specifically, some reasoned method for selecting and counting the most interesting compounds must be employed. In CFQ, the chosen method was comprised of two steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Candidate sub-graph generation. From $T$, first generate some large sub-set of candidate sub-graphs, called $\mathbb{G}$. The paper considered all sub-graphs with up to 5 nodes with
branching factor up to 2, plus linear sub-graphs of all sizes (from personal correspondence with the authors).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Candidate ranking: As written in sec. 2.1, the frequency of many of these sub-graphs will be highly correlated with their super-graphs, leading to the undesirable possibility of double counting sub-graphs. Accordingly, a further weighting step is carried out, by which a weight is computed for each $G\in\mathbb{G}$. This weight takes into account each occurrence of $G$ in the sample set, so the final weight (un-normalized probability) of $G$ is given by $W\left(G\right)=\sum_{R\in T}w\left(G,R\right)$. The weight per sample $R$, $w\left(G,R\right)$, is calculated by:&lt;/p&gt;
&lt;p&gt;$$w\left(G,R\right)=\max_{g\in\text{occ}\left(G,R\right)}\left(1-\max_{G&#39;:g\prec g&#39;\in\text{occ}\left(G&#39;,R\right)}P\left(G&#39;|G\right)\right)$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s unpack what&amp;rsquo;s going on in this calculation (see also appendix L.4). $\text{occ}\left(G,R\right)$ is the set of all occurrences of sub-graph $G$ in sample $R$ (one sub-graph may appear multiple times). The $\prec$ denotes the strict sub-graph relation. $P\left(G&#39;|G\right)$ is the empirical probability of $G&#39;$ occurring as a super-graph of $G$ over the whole sample set $T$.  So for each such occurrence $g$, we look at all super-graphs of $g$ in $R$, and take the super-graph $G&#39;$ which co-occurs most often with $G$ over $T$.&lt;/p&gt;
&lt;p&gt;Intuitively, $w\left(G,R\right)$ estimates how interesting $G$ is in the context of $R$. The higher the probability of some super-graph $G&#39;$ co-occurring with $G$ across $T$, the less interesting $G$ is. So for each occurrence $g$, we find its weight by taking the complement of this maximum empirical probability. To calculate the weight of $G$, we take the weight of the maximally interesting occurrence (the outer max).&lt;/p&gt;
&lt;p&gt;This can be a little confusing, so let&amp;rsquo;s make it more concrete with a small example. Consider a toy sample set $T=\left\{ R_{1},R_{2},R_{3}\right\}$ and some sample sub-graphs $\mathbb{G}=\left\{ G_{1},G_{2},&amp;hellip;\right\}$  as depicted below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;media/dbca.png&#34; alt=&#34;media/dbca.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;For this case, we would have $W\left(G_{1},R_{1}\right)=W\left(G_{1},R_{2}\right)=W\left(G_{1},R_{3}\right)=0$, since $G_1$ occurs only within the context of the super-graph $G_2$ across all samples. Conversely, $W\left(G_{2},R_{1}\right)=W\left(G_{2},R_{2}\right)=W\left(G_{2},R_{3}\right)=1$, since $G_2$ appears in a different context across $T$- there is no common super-graph of $G_2$ across all samples. These of course are extreme cases just to show the point of the weighting scheme, which&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ensures that when calculating compound divergence based on a weighted subset of compounds, the most representative compounds are taken into account, while avoiding double-counting compounds whose frequency of occurrence is already largely explainable by the frequency of occurrence of one of its super-compounds.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Compound weight:&lt;/strong&gt; Given the weights as defined above, we can calculate the un-normalized weight of a compound $G$ as $W\left(G\right)=\sum_{R\in T}W\left(G,R\right)$. We assume that $W\left(G,R\right)=0$ if $G$ doesn&amp;rsquo;t occur in $R$.&lt;/p&gt;
&lt;p&gt;In practice, in CFQ the graphs in $\mathbb{G}$ are then sorted according to descending $W$ and the top 100,000 are kept.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Probability of a compound:&lt;/strong&gt; To get the normalized probability, we can simply divide by the total sum over all of the weights for all compounds in the sample set:&lt;/p&gt;
&lt;p&gt;$$P\left(G\right)=\frac{W\left(G\right)}{\sum_{G&#39;\in\mathbb{G}}W\left(G&#39;\right)}$$&lt;/p&gt;
&lt;h3 id=&#34;divergence&#34;&gt;Divergence&lt;/h3&gt;
&lt;p&gt;Now we have all of the definitions in place to define distance (or divergence) between our train and test splits. DBCA uses the Chernoff coefficient which yields a scalar between 0 and 1, given two distributions $P$ and $Q$: $C_{\alpha}\left(P\parallel Q\right)=\sum_{k}p_{k}^{\alpha}q_{k}^{1-\alpha}$. The more similar two distributions are, the closer this score will be to 1.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re measuring divergence though, so we take the complement to compute atom and compound divergences, as follows:
$$\mathcal{D}_{A}\left(V\parallel W\right)=1 - C_{0.5}\left(\mathcal{F}_{A}\left(V\right)\parallel\mathcal{F}_{A}\left(W\right)\right)$$&lt;/p&gt;
&lt;p&gt;$$\mathcal{D}_{C}\left(V\parallel W\right)=1 - C_{0.1}\left(\mathcal{F}_{C}\left(V\right)\parallel\mathcal{F}_{C}\left(W\right)\right)$$&lt;/p&gt;
&lt;p&gt;Where $V$ and $W$ are train and test sets, respectively.&lt;/p&gt;
&lt;p&gt;What is $\alpha$ for? It serves to control the relative weight given to the train and test splits. As written in sec. 2.1 of the paper:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For the atom divergence, we use α = 0.5, which &amp;hellip; reflects the desire of making the atom distributions in train and test as similar as possible. For the compound divergence, we use α = 0.1, which reflects the intuition that it is more important whether a certain compound occurs in P (train) than whether the probabilities in P (train) and Q (test) match exactly.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let&amp;rsquo;s make this more concrete with a toy example, assuming train and test distributions over just 4 compounds:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;p = np.array([0.001, 0.001, 1-0.002, 0])
q = np.array([0.4,0.4,0,0.2])
C_alpha_1 = np.dot(p**0.1, q**0.9) # -&amp;gt; C_alpha_1 = 0.44
C_alpha_5 = np.dot(p**0.5, q**0.5) # -&amp;gt; C_alpha_5 = 0.04
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the mere existence in train set $P$ of even small counts of the first 2 compounds yields a high $C_{0.1}$-similarity with $Q$, where $C_{0.5}$ would only be high if the distributions were actually similar.&lt;/p&gt;
&lt;p&gt;Armed with these definitions, let&amp;rsquo;s see how they can be used in practice to generate train and test sets with desired atom and compound divergences.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;generating-splits&#34;&gt;Generating Splits&lt;/h1&gt;
&lt;p&gt;The DBCA paper presents a pretty straightforward way to generate splits with some desired atom and compound divergences. The basic idea is to start with some pool of samples $U$, pre-compute the weights $W\left(G,R\right)$ for all &lt;a href=&#34;&#34;&gt;considered sub-graphs&lt;/a&gt;, and then iteratively add a new sample to the train/test sets such that $\mathcal{D}_{C}$ and $\mathcal{D}_{A}$ are closest to the desired values $d_a$, $d_c$ (while also maintaining the train/test set size ratio). The paper doesn&amp;rsquo;t describe the greedy step explicitly, so in my implementation, in each iteration, I just add the sample minimizing the score function:&lt;/p&gt;
&lt;p&gt;$$S\left(V,W\right)=\left|\mathcal{D}_{C}\left(V\parallel W\right)-d_{c}\right|+\left|\mathcal{D}_{A}\left(V\parallel W\right)-d_{a}\right|$$&lt;/p&gt;
&lt;p&gt;Another issue is that since this is a greedy algorithm, the paper notes that random restarts may be required by removing samples at certain iterations. There are no further details on this, and I haven&amp;rsquo;t yet incorporated it into my implementation.&lt;/p&gt;
&lt;p&gt;So let&amp;rsquo;s look at the basic pseudocode to get a feel for the flow.&lt;/p&gt;
&lt;h2 id=&#34;pseudocode&#34;&gt;Pseudocode&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Inputs:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$U$: a pool of samples.&lt;/li&gt;
&lt;li&gt;$d_a$, $d_c$: Target atom and compound divergences (in [0,1]).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Algorithm:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize empty sets $V$ (train) and $W$ (test).&lt;/li&gt;
&lt;li&gt;For each of the chosen candidate sub-graph types $G\in\mathbb{G}$ and for each sample $u$ in $U$, calculate $W\left(G,u\right)$, and retain the top $N$ sub-graphs (in terms of $W\left(G\right)$).&lt;/li&gt;
&lt;li&gt;Select random sample from $U$ and add to $V$. // initialization&lt;/li&gt;
&lt;li&gt;For  $i=1&amp;hellip;(T-1)$:
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;If $\text{is-train-step}\left(i\right)$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$u^*=\arg\min_{u\in U}\left(S\left(V^{(i-1)}\cup\left\{ u^*\right\} ,W^{(i-1)}\right)\right)$&lt;/li&gt;
&lt;li&gt;$V^{(i)}=V^{(i-1)}\cup \left\{ u^* \right\}, W^{(i)}=W^{(i-1)} $&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;else: // test step&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$u^*=\arg\min_{u\in U}\left(S\left(V^{(i-1)} ,W^{(i-1)}\cup\left\{ u^* \right\}\right)\right)$&lt;/li&gt;
&lt;li&gt;$W^{(i)}=W^{(i-1)}\cup\left\{ u^*\right\},V^{(i)}=V^{(i-1)}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;open-questions&#34;&gt;Open Questions&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;DBCA is reliant on the ability to generate data in a controllable way, thus requiring some kind of simulator or grammar. An interesting question is how to incorporate it with approaches for more natural language, like crowdsourcing or NLG.&lt;/li&gt;
&lt;li&gt;The current approach addresses short question/answer pairs, I wonder how it would scale to larger and more complex graphs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;repo&#34;&gt;Repo&lt;/h1&gt;
&lt;p&gt;Feel free to check out (and improve) my implementation at &lt;a href=&#34;https://github.com/ronentk/dbca-splitter&#34;&gt;https://github.com/ronentk/dbca-splitter&lt;/a&gt; !&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Language (Re)modelling: Towards Embodied Language Understanding (blog version)</title>
      <link>https://ronentk.github.io/post/acl2020-blog/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/post/acl2020-blog/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;A SONG of the rolling earth, and of words according,&lt;/p&gt;
&lt;p&gt;Were you thinking that those were the words, those upright lines?
those curves, angles, dots?&lt;/p&gt;
&lt;p&gt;No, those are not the words, the substantial words are in the
ground and sea,&lt;/p&gt;
&lt;p&gt;They are in the air, they are in you.&amp;rdquo; &amp;ndash; Walt Whitman&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Natural language processing (NLP) has become one of AI’s hottest research areas.  Deep learning algorithms drawing upon massive amounts of data and specialized hardware have achieved impressive empirical progress, powering applications from multilingual machine translation to wise-cracking chatbots.
However, state-of-the-art NLP algorithms continue to struggle with tasks that schoolchildren find trivial. For example, consider the following story: “John dropped the wine glass on the coffee table and watched with dismay as it shattered into pieces.”&lt;/p&gt;
&lt;p&gt;Most people would easily identify “it” as referring to the wine glass. However, current NLP algorithms find this deduction difficult; most algorithms we tested judged it likely that both the wine glass and the coffee table shattered into pieces. The situation becomes even more complex with sentences whose meaning is not literal, such as “John’s career hopes shattered.”&lt;/p&gt;
&lt;p&gt;Perhaps the problem is the way that these algorithms learn about the world. Typically, NLP algorithms read terabytes of texts, extracting statistical patterns of language use from them. However, research in the Cognitive Sciences, and specifically &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/j.1756-8765.2012.01222.x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Embodied Cognitive Linguistics&lt;/a&gt; (ECL), argues that when people communicate, a lot of the meaning is not even present in the words. Instead, ECL claims that people use embodied world knowledge to understand language, both literal (wine glasses) and non-literal (career hopes). &lt;em&gt;Embodied&lt;/em&gt; means such knowledge is deeply dependent on features of human physical bodies, such as the abilities of locomotion, perception, and emotion.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2020.acl-main.559/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Language (Re)modelling: Towards Embodied Language Understanding&lt;/a&gt; aims to integrate ideas from ECL into current NLP research. The work highlights two crucial cognitive capabilities missing in today’s NLP methods: mental simulation and metaphoric interpretation.&lt;/p&gt;
&lt;p&gt;According to ECL’s &lt;a href=&#34;http://www.cogsci.ucsd.edu/~bkbergen/papers/ESM.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Simulation Hypothesis&lt;/a&gt;, language users come to the text armed with vast world knowledge and a powerful imagination that facilitates conjuring detailed simulations of the world to achieve language understanding, such as knowing what happens when a wine glass drops. To simulate a more abstract concept like “career hopes”, ECL’s &lt;a href=&#34;https://en.wikipedia.org/wiki/Conceptual_metaphor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Conceptual Metaphor Theory&lt;/a&gt; hypothesizes that, through metaphor, humans creatively construe more abstract concepts in terms of more concrete concepts. In other words, language users imagine concepts that are hard to simulate (career hopes) in terms of those that are easier to simulate. Construing career hopes as a physical object makes sense, since their “breaking” evokes the notion that they may be hard to recover.&lt;/p&gt;
&lt;p&gt;Importantly, both mental simulation and metaphoric interpretation derive from interaction in the world rather than via static text. Accordingly, the paper argues for designing cognitively-inspired architectures, including diverse simulation environments through which the next generation of AI agents can begin learning the world knowledge necessary for more human-like language understanding.
Ultimately, the deeply embodied nature of language implies that we needn’t worry about AI poets surpassing humans in writing skill anytime soon. Rather, it suggests that a tighter integration between contemporary cognitive science and NLP research is a promising approach towards AI assistants with a better understanding of human language.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Talk at Tel Aviv University NLP Seminar</title>
      <link>https://ronentk.github.io/talk/tau_seminar_0420/</link>
      <pubDate>Thu, 30 Apr 2020 13:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/talk/tau_seminar_0420/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Language (Re)modelling: Towards Embodied Language Understanding</title>
      <link>https://ronentk.github.io/publication/tamari-2020-remodelling/</link>
      <pubDate>Mon, 13 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/publication/tamari-2020-remodelling/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Talk at Bar Ilan University NLP Seminar</title>
      <link>https://ronentk.github.io/talk/biu_seminar_0320/</link>
      <pubDate>Sun, 29 Mar 2020 11:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/talk/biu_seminar_0320/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Ecological Semantics: Programming Environments for Situated Language Understanding</title>
      <link>https://ronentk.github.io/publication/tamari-2020-ecological/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/publication/tamari-2020-ecological/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Popular science piece on embodied cognition and AI (Hebrew).</title>
      <link>https://ronentk.github.io/post/bac/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/post/bac/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.bac.org.il/society/article/bgvph-anhnv-mbynym&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;External link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Poster at AI Week Israel 2019</title>
      <link>https://ronentk.github.io/talk/ai_week_1119/</link>
      <pubDate>Mon, 18 Nov 2019 15:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/talk/ai_week_1119/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Y′all should read this! Identifying Plurality in Second-Person Personal Pronouns in English Texts</title>
      <link>https://ronentk.github.io/publication/stanovsky-tamari-2019-yall/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/publication/stanovsky-tamari-2019-yall/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Playing by the Book: An Interactive Game Approach for Action Graph Extraction from Text</title>
      <link>https://ronentk.github.io/publication/tamari-2018/</link>
      <pubDate>Sun, 02 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/publication/tamari-2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Talk at Tel Aviv Data Science Meetup</title>
      <link>https://ronentk.github.io/talk/ds_meetup_0419/</link>
      <pubDate>Fri, 19 Apr 2019 18:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/talk/ds_meetup_0419/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Talk at JerusML Meetup</title>
      <link>https://ronentk.github.io/talk/jerusml_meetup_0119/</link>
      <pubDate>Thu, 03 Jan 2019 20:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/talk/jerusml_meetup_0119/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Fair Consensus Protocol for Transaction Ordering</title>
      <link>https://ronentk.github.io/publication/asayag-2018/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/publication/asayag-2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Boosting Dilated Convolutional Networks with Mixed Tensor Decompositions</title>
      <link>https://ronentk.github.io/publication/cohen-2018-boosting/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/publication/cohen-2018-boosting/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Tensorial Mixture Models</title>
      <link>https://ronentk.github.io/publication/sharir-2016-a/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/publication/sharir-2016-a/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
