<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ronen Tamari</title>
    <link>https://ronentk.github.io/</link>
      <atom:link href="https://ronentk.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Ronen Tamari</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 16 Apr 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://ronentk.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Ronen Tamari</title>
      <link>https://ronentk.github.io/</link>
    </image>
    
    <item>
      <title>Process-Level Representation of Scientific Protocols with a Text-Based Game Annotation Interface</title>
      <link>https://ronentk.github.io/publication/tamari-2020-process-eacl/</link>
      <pubDate>Fri, 16 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/publication/tamari-2020-process-eacl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Seminar Talk at AI2 - Aristo</title>
      <link>https://ronentk.github.io/talk/aristo_0121/</link>
      <pubDate>Tue, 12 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/talk/aristo_0121/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Contributed Talk (WordPlay @ NeurIPS2020)</title>
      <link>https://ronentk.github.io/talk/wordplay_2020/</link>
      <pubDate>Wed, 09 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/talk/wordplay_2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>üîç Managing Research in Notion!</title>
      <link>https://ronentk.github.io/post/notion/</link>
      <pubDate>Sun, 06 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/post/notion/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.notion.so/ronhome/Managing-Research-in-Notion-9cb021e748884b2f8936ffb98cca58a3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;External link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Process-Level Representation of Scientific Protocols with a Text-Based Game Annotation Interface</title>
      <link>https://ronentk.github.io/publication/tamari-2020-process-wp/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/publication/tamari-2020-process-wp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Poster at NAISys Virtual Conference</title>
      <link>https://ronentk.github.io/talk/naisys_1120/</link>
      <pubDate>Mon, 09 Nov 2020 13:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/talk/naisys_1120/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Distribution Based Compositionality Assessment (DBCA)</title>
      <link>https://ronentk.github.io/post/dbca/</link>
      <pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/post/dbca/</guid>
      <description>&lt;h1 id=&#34;background-compositional-generalizations&#34;&gt;Background: Compositional Generalizations&lt;/h1&gt;
&lt;p&gt;Compositional generalization, or models&#39; lack of it, is a hot topic in NLP right now. Compositional generalization (&lt;a href=&#34;http://ruccs.rutgers.edu/images/personal-zenon-pylyshyn/proseminars/Proseminar13/ConnectionistArchitecture.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fodor &amp;amp; Pylysyn, 1988&lt;/a&gt;) refers to the ability to make &amp;ldquo;infinite use of finite means&amp;rdquo; and is a cornerstone of human intelligence in general, and in particular language use: humans can combine a finite set of discrete elements (such as words) in limitless ways to create new meanings. An oft-quoted &lt;a href=&#34;https://arxiv.org/abs/1711.00350&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;example&lt;/a&gt; is that once humans learn the meaning of a new verb like &amp;ldquo;dax&amp;rdquo;, we effortlessly generalize to new novel combinations with known words, like &amp;ldquo;dax twice&amp;rdquo; or &amp;ldquo;dax slowly&amp;rdquo;. While crucial for more robust and efficient NLP, compositional generalization remains extremely challenging for state-of-the-art (SOTA) models, with a &lt;a href=&#34;http://arxiv.org/abs/2007.08970&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recent paper&lt;/a&gt; noting that despite the huge investment,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;General ML architecture improvements yield incremental, if limited, improvements in compositional generalization settings.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;How to even measure the ability for compositional generalization? Like so many concepts in natural language, intuitive explanations of compositional generalization are easy, but more precise definitions can be notoriously elusive. Many works (e.g., &lt;a href=&#34;https://arxiv.org/abs/1711.00350&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SCAN&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2003.05161&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gSCAN&lt;/a&gt;) adopt a simple approach of holding out some set of test samples which differ systematically from those seen in training. For example, showing a model what it means to &amp;ldquo;walk while spinning&amp;rdquo; and &amp;ldquo;push the circle&amp;rdquo; at train time, and then testing it on &amp;ldquo;push the circle while spinning&amp;rdquo;. While such approaches indeed test for compositional generalization, a limitation is that the holding out classes of samples is somewhat heuristic and hand-engineered, and doesn&amp;rsquo;t yield a precise, numeric or canonical quantification of train-test differences.&lt;/p&gt;
&lt;p&gt;Fortunately, a cool &lt;a href=&#34;https://openreview.net/forum?id=SygcCnNKwr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; presented this year at ICLR2020 makes some nice progress on this question. The paper, out of Google Brain, is called &lt;em&gt;&amp;ldquo;Measuring Compositional Generalization: A Comprehensive Method on Realistic Data&amp;rdquo;&lt;/em&gt;. They propose a method, Distribution Based Compositionality Assessment (DBCA), which allows constructing and more precisely quantifying the compositional-generalization gap between train and test splits.&lt;/p&gt;
&lt;p&gt;I really liked the paper and indeed found it comprehensive (including a 20+ page appendix), so this is my attempt at a walk-through and accompaniment to my (un-offical) &lt;a href=&#34;https://github.com/ronentk/dbca-splitter&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;re-implementation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Turn the dial on the train-test compositionality gap, and watch model performance drop! If it doesn&amp;rsquo;t: either you have a breakthrough, or I have a bug üôÉ.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;media/dbca.gif&#34; alt=&#34;media/dbca.gif&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;dbca-in-a-nutshell&#34;&gt;DBCA in a nutshell&lt;/h1&gt;
&lt;p&gt;DBCA provides a method of systematically generating datasets with train and test splits diverging in a controllable and measurable way. The paper applies this method to generate a compositional question answering dataset called CFQ (Compositional Freebase Questions). They show that turning the divergence dial strongly corresponds with a decrease in model accuracy, while remaining compositional and human interpretable; A human who could correctly answer the training set examples of ‚ÄúWho directed Inception?‚Äù and ‚ÄúDid Christopher Nolan produce Goldfinger?‚Äù would have no trouble testing on questions such as ‚ÄúDid Christopher Nolan direct Goldfinger?‚Äù and &amp;ldquo;Who produced Inception?&amp;rdquo;, but current SOTA models fail dramatically in such settings.&lt;/p&gt;
&lt;p&gt;As the name implies, the magic happens by carefully controlling two distribution measures between train and test splits:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Atom Divergence: we&amp;rsquo;ll get formal later, but for now we can intuitively think of atoms as the elementary building blocks comprising each sample:  predicates liked &amp;ldquo;directed&amp;rdquo;, or entities like &amp;ldquo;Inception&amp;rdquo;. We want the atoms to be distributed roughly the same across train and test distributions‚Üílow atom divergence, as shown in Fig. 6 of the paper&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;media/fig6a.png&#34; alt=&#34;media/fig6a.png&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compound Divergence: Compounds represent different ways of composing atoms. ‚ÄúDid Christopher Nolan direct Goldfinger?‚Äù and &amp;ldquo;Who produced Inception?&amp;rdquo; are novel compounds over the same atoms from the training set (&amp;ldquo;directed&amp;rdquo;, &amp;ldquo;Inception&amp;rdquo;, etc). To measure compositionality, we want to create test sets with novel compounds not seen at train time:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;media/fig6c.png&#34; alt=&#34;media/fig6c.png&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;definitions&#34;&gt;Definitions&lt;/h1&gt;
&lt;h2 id=&#34;sample-definition&#34;&gt;Sample Definition&lt;/h2&gt;
&lt;p&gt;To meaningfully measure how samples differ from each other, let&amp;rsquo;s first understand how a sample is defined and generated in DBCA. Each sample is defined to be a directed a-cyclic graph (DAG), where nodes (or atoms) correspond to rule applications. Edges correspond to dependencies between rule applications.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nodes:&lt;/strong&gt; In CFQ, the paper mentions 4 kinds of rules used in question generation: grammar (211), inference (17), resolution (21) and knowledge (194). This yields a total of $N_{A}=443$ types of nodes, as can be seen in the histogram in Fig. 6 (above). Feel free to check out the paper for specific rule examples: this post is less about the particulars of CFQ and more the general workings of DBCA.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Edges:&lt;/strong&gt; DAG edges represent dependencies among the rules (nodes), an edge $A \to B$ means that rule $B$ strictly depends on rule $A$ in the sense that the generator cannot apply rule $B$ before applying rule $A$.&lt;/p&gt;
&lt;p&gt;Comparing graphs is known to be hard (graph isomorphism problem), therefore the DAG is normalized to ensure that a certain rule combination is represented using the same DAG across all the examples where it occurs. This is important for meaningfully comparing measures such as divergence.&lt;/p&gt;
&lt;h2 id=&#34;distributions-and-divergences&#34;&gt;Distributions and Divergences&lt;/h2&gt;
&lt;h3 id=&#34;atom-distribution&#34;&gt;Atom Distribution&lt;/h3&gt;
&lt;p&gt;For a sample set $T$, the &lt;em&gt;atom distribution,&lt;/em&gt;  $\mathcal{F}_{A}\left(T\right)$, is defined to be the frequency distribution of atoms in the set.&lt;/p&gt;
&lt;p&gt;Here $T$ is a collection of DAGs, and atoms correspond to graph node types. To be more concrete, you can imagine $\mathcal{F}_{A} \left( T \right)$ being represented by a $N_{A}$ dimensional vector containing the normalized frequency counts of each atom (rule application) across $T$.&lt;/p&gt;
&lt;p&gt;As mentioned, DBCA creates training and test sets which share &lt;strong&gt;similar&lt;/strong&gt; atom distributions.&lt;/p&gt;
&lt;h3 id=&#34;compound-distribution&#34;&gt;Compound Distribution&lt;/h3&gt;
&lt;p&gt;For a sample set $T$, the &lt;em&gt;compound distribution,&lt;/em&gt; $\mathcal{F}_{C}\left(T\right)$, is defined to be the frequency distribution of compounds in the set.&lt;/p&gt;
&lt;p&gt;In contrast to the atom distribution, we want the compound distribution to be divergent across train and test sets, in order to test models&#39; ability to recombine known atoms in novel ways. To represent $\mathcal{F}_{C}\left(T\right)$, you can similarly imagine a long vector containing the frequency counts of each &lt;em&gt;compound&lt;/em&gt; in $T$. Here we run into a problem though, due to combinatorial complexity considerations- the number of all possible compounds (sub-graphs) is generally exponential in the size of $T$.&lt;/p&gt;
&lt;p&gt;Therefore, for practical purposes, the compound distribution requires a more involved definition. Specifically, some reasoned method for selecting and counting the most interesting compounds must be employed. In CFQ, the chosen method was comprised of two steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Candidate sub-graph generation. From $T$, first generate some large sub-set of candidate sub-graphs, called $\mathbb{G}$. The paper considered all sub-graphs with up to 5 nodes with
branching factor up to 2, plus linear sub-graphs of all sizes (from personal correspondence with the authors).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Candidate ranking: As written in sec. 2.1, the frequency of many of these sub-graphs will be highly correlated with their super-graphs, leading to the undesirable possibility of double counting sub-graphs. Accordingly, a further weighting step is carried out, by which a weight is computed for each $G\in\mathbb{G}$. This weight takes into account each occurrence of $G$ in the sample set, so the final weight (un-normalized probability) of $G$ is given by $W\left(G\right)=\sum_{R\in T}w\left(G,R\right)$. The weight per sample $R$, $w\left(G,R\right)$, is calculated by:&lt;/p&gt;
&lt;p&gt;$$w\left(G,R\right)=\max_{g\in\text{occ}\left(G,R\right)}\left(1-\max_{G&#39;:g\prec g&#39;\in\text{occ}\left(G&#39;,R\right)}P\left(G&#39;|G\right)\right)$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s unpack what&amp;rsquo;s going on in this calculation (see also appendix L.4). $\text{occ}\left(G,R\right)$ is the set of all occurrences of sub-graph $G$ in sample $R$ (one sub-graph may appear multiple times). The $\prec$ denotes the strict sub-graph relation. $P\left(G&#39;|G\right)$ is the empirical probability of $G&#39;$ occurring as a super-graph of $G$ over the whole sample set $T$.  So for each such occurrence $g$, we look at all super-graphs of $g$ in $R$, and take the super-graph $G&#39;$ which co-occurs most often with $G$ over $T$.&lt;/p&gt;
&lt;p&gt;Intuitively, $w\left(G,R\right)$ estimates how interesting $G$ is in the context of $R$. The higher the probability of some super-graph $G&#39;$ co-occurring with $G$ across $T$, the less interesting $G$ is. So for each occurrence $g$, we find its weight by taking the complement of this maximum empirical probability. To calculate the weight of $G$, we take the weight of the maximally interesting occurrence (the outer max).&lt;/p&gt;
&lt;p&gt;This can be a little confusing, so let&amp;rsquo;s make it more concrete with a small example. Consider a toy sample set $T=\left\{ R_{1},R_{2},R_{3}\right\}$ and some sample sub-graphs $\mathbb{G}=\left\{ G_{1},G_{2},&amp;hellip;\right\}$  as depicted below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;media/dbca.png&#34; alt=&#34;media/dbca.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;For this case, we would have $W\left(G_{1},R_{1}\right)=W\left(G_{1},R_{2}\right)=W\left(G_{1},R_{3}\right)=0$, since $G_1$ occurs only within the context of the super-graph $G_2$ across all samples. Conversely, $W\left(G_{2},R_{1}\right)=W\left(G_{2},R_{2}\right)=W\left(G_{2},R_{3}\right)=1$, since $G_2$ appears in a different context across $T$- there is no common super-graph of $G_2$ across all samples. These of course are extreme cases just to show the point of the weighting scheme, which&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ensures that when calculating compound divergence based on a weighted subset of compounds, the most representative compounds are taken into account, while avoiding double-counting compounds whose frequency of occurrence is already largely explainable by the frequency of occurrence of one of its super-compounds.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Compound weight:&lt;/strong&gt; Given the weights as defined above, we can calculate the un-normalized weight of a compound $G$ as $W\left(G\right)=\sum_{R\in T}W\left(G,R\right)$. We assume that $W\left(G,R\right)=0$ if $G$ doesn&amp;rsquo;t occur in $R$.&lt;/p&gt;
&lt;p&gt;In practice, in CFQ the graphs in $\mathbb{G}$ are then sorted according to descending $W$ and the top 100,000 are kept.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Probability of a compound:&lt;/strong&gt; To get the normalized probability, we can simply divide by the total sum over all of the weights for all compounds in the sample set:&lt;/p&gt;
&lt;p&gt;$$P\left(G\right)=\frac{W\left(G\right)}{\sum_{G&#39;\in\mathbb{G}}W\left(G&#39;\right)}$$&lt;/p&gt;
&lt;h3 id=&#34;divergence&#34;&gt;Divergence&lt;/h3&gt;
&lt;p&gt;Now we have all of the definitions in place to define distance (or divergence) between our train and test splits. DBCA uses the Chernoff coefficient which yields a scalar between 0 and 1, given two distributions $P$ and $Q$: $C_{\alpha}\left(P\parallel Q\right)=\sum_{k}p_{k}^{\alpha}q_{k}^{1-\alpha}$. The more similar two distributions are, the closer this score will be to 1.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re measuring divergence though, so we take the complement to compute atom and compound divergences, as follows:
$$\mathcal{D}_{A}\left(V\parallel W\right)=1 - C_{0.5}\left(\mathcal{F}_{A}\left(V\right)\parallel\mathcal{F}_{A}\left(W\right)\right)$$&lt;/p&gt;
&lt;p&gt;$$\mathcal{D}_{C}\left(V\parallel W\right)=1 - C_{0.1}\left(\mathcal{F}_{C}\left(V\right)\parallel\mathcal{F}_{C}\left(W\right)\right)$$&lt;/p&gt;
&lt;p&gt;Where $V$ and $W$ are train and test sets, respectively.&lt;/p&gt;
&lt;p&gt;What is $\alpha$ for? It serves to control the relative weight given to the train and test splits. As written in sec. 2.1 of the paper:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For the atom divergence, we use Œ± = 0.5, which &amp;hellip; reflects the desire of making the atom distributions in train and test as similar as possible. For the compound divergence, we use Œ± = 0.1, which reflects the intuition that it is more important whether a certain compound occurs in P (train) than whether the probabilities in P (train) and Q (test) match exactly.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let&amp;rsquo;s make this more concrete with a toy example, assuming train and test distributions over just 4 compounds:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;p = np.array([0.001, 0.001, 1-0.002, 0])
q = np.array([0.4,0.4,0,0.2])
C_alpha_1 = np.dot(p**0.1, q**0.9) # -&amp;gt; C_alpha_1 = 0.44
C_alpha_5 = np.dot(p**0.5, q**0.5) # -&amp;gt; C_alpha_5 = 0.04
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the mere existence in train set $P$ of even small counts of the first 2 compounds yields a high $C_{0.1}$-similarity with $Q$, where $C_{0.5}$ would only be high if the distributions were actually similar.&lt;/p&gt;
&lt;p&gt;Armed with these definitions, let&amp;rsquo;s see how they can be used in practice to generate train and test sets with desired atom and compound divergences.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;generating-splits&#34;&gt;Generating Splits&lt;/h1&gt;
&lt;p&gt;The DBCA paper presents a pretty straightforward way to generate splits with some desired atom and compound divergences. The basic idea is to start with some pool of samples $U$, pre-compute the weights $W\left(G,R\right)$ for all &lt;a href=&#34;&#34;&gt;considered sub-graphs&lt;/a&gt;, and then iteratively add a new sample to the train/test sets such that $\mathcal{D}_{C}$ and $\mathcal{D}_{A}$ are closest to the desired values $d_a$, $d_c$ (while also maintaining the train/test set size ratio). The paper doesn&amp;rsquo;t describe the greedy step explicitly, so in my implementation, in each iteration, I just add the sample minimizing the score function:&lt;/p&gt;
&lt;p&gt;$$S\left(V,W\right)=\left|\mathcal{D}_{C}\left(V\parallel W\right)-d_{c}\right|+\left|\mathcal{D}_{A}\left(V\parallel W\right)-d_{a}\right|$$&lt;/p&gt;
&lt;p&gt;Another issue is that since this is a greedy algorithm, the paper notes that random restarts may be required by removing samples at certain iterations. There are no further details on this, and I haven&amp;rsquo;t yet incorporated it into my implementation.&lt;/p&gt;
&lt;p&gt;So let&amp;rsquo;s look at the basic pseudocode to get a feel for the flow.&lt;/p&gt;
&lt;h2 id=&#34;pseudocode&#34;&gt;Pseudocode&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Inputs:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$U$: a pool of samples.&lt;/li&gt;
&lt;li&gt;$d_a$, $d_c$: Target atom and compound divergences (in [0,1]).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Algorithm:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize empty sets $V$ (train) and $W$ (test).&lt;/li&gt;
&lt;li&gt;For each of the chosen candidate sub-graph types $G\in\mathbb{G}$ and for each sample $u$ in $U$, calculate $W\left(G,u\right)$, and retain the top $N$ sub-graphs (in terms of $W\left(G\right)$).&lt;/li&gt;
&lt;li&gt;Select random sample from $U$ and add to $V$. // initialization&lt;/li&gt;
&lt;li&gt;For  $i=1&amp;hellip;(T-1)$:
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;If $\text{is-train-step}\left(i\right)$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$u^*=\arg\min_{u\in U}\left(S\left(V^{(i-1)}\cup\left\{ u^*\right\} ,W^{(i-1)}\right)\right)$&lt;/li&gt;
&lt;li&gt;$V^{(i)}=V^{(i-1)}\cup \left\{ u^* \right\}, W^{(i)}=W^{(i-1)} $&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;else: // test step&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$u^*=\arg\min_{u\in U}\left(S\left(V^{(i-1)} ,W^{(i-1)}\cup\left\{ u^* \right\}\right)\right)$&lt;/li&gt;
&lt;li&gt;$W^{(i)}=W^{(i-1)}\cup\left\{ u^*\right\},V^{(i)}=V^{(i-1)}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;open-questions&#34;&gt;Open Questions&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;DBCA is reliant on the ability to generate data in a controllable way, thus requiring some kind of simulator or grammar. An interesting question is how to incorporate it with approaches for more natural language, like crowdsourcing or NLG.&lt;/li&gt;
&lt;li&gt;The current approach addresses short question/answer pairs, I wonder how it would scale to larger and more complex graphs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;repo&#34;&gt;Repo&lt;/h1&gt;
&lt;p&gt;Feel free to check out (and improve) my implementation at &lt;a href=&#34;https://github.com/ronentk/dbca-splitter&#34;&gt;https://github.com/ronentk/dbca-splitter&lt;/a&gt; !&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Language (Re)modelling: Towards Embodied Language Understanding (blog version)</title>
      <link>https://ronentk.github.io/post/acl2020-blog/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/post/acl2020-blog/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;A SONG of the rolling earth, and of words according,&lt;/p&gt;
&lt;p&gt;Were you thinking that those were the words, those upright lines?
those curves, angles, dots?&lt;/p&gt;
&lt;p&gt;No, those are not the words, the substantial words are in the
ground and sea,&lt;/p&gt;
&lt;p&gt;They are in the air, they are in you.&amp;rdquo; &amp;ndash; Walt Whitman&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Natural language processing (NLP) has become one of AI‚Äôs hottest research areas.  Deep learning algorithms drawing upon massive amounts of data and specialized hardware have achieved impressive empirical progress, powering applications from multilingual machine translation to wise-cracking chatbots.
However, state-of-the-art NLP algorithms continue to struggle with tasks that schoolchildren find trivial. For example, consider the following story: ‚ÄúJohn dropped the wine glass on the coffee table and watched with dismay as it shattered into pieces.‚Äù&lt;/p&gt;
&lt;p&gt;Most people would easily identify ‚Äúit‚Äù as referring to the wine glass. However, current NLP algorithms find this deduction difficult; most algorithms we tested judged it likely that both the wine glass and the coffee table shattered into pieces. The situation becomes even more complex with sentences whose meaning is not literal, such as ‚ÄúJohn‚Äôs career hopes shattered.‚Äù&lt;/p&gt;
&lt;p&gt;Perhaps the problem is the way that these algorithms learn about the world. Typically, NLP algorithms read terabytes of texts, extracting statistical patterns of language use from them. However, research in the Cognitive Sciences, and specifically &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/j.1756-8765.2012.01222.x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Embodied Cognitive Linguistics&lt;/a&gt; (ECL), argues that when people communicate, a lot of the meaning is not even present in the words. Instead, ECL claims that people use embodied world knowledge to understand language, both literal (wine glasses) and non-literal (career hopes). &lt;em&gt;Embodied&lt;/em&gt; means such knowledge is deeply dependent on features of human physical bodies, such as the abilities of locomotion, perception, and emotion.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2020.acl-main.559/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Language (Re)modelling: Towards Embodied Language Understanding&lt;/a&gt; aims to integrate ideas from ECL into current NLP research. The work highlights two crucial cognitive capabilities missing in today‚Äôs NLP methods: mental simulation and metaphoric interpretation.&lt;/p&gt;
&lt;p&gt;According to ECL‚Äôs &lt;a href=&#34;http://www.cogsci.ucsd.edu/~bkbergen/papers/ESM.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Simulation Hypothesis&lt;/a&gt;, language users come to the text armed with vast world knowledge and a powerful imagination that facilitates conjuring detailed simulations of the world to achieve language understanding, such as knowing what happens when a wine glass drops. To simulate a more abstract concept like ‚Äúcareer hopes‚Äù, ECL‚Äôs &lt;a href=&#34;https://en.wikipedia.org/wiki/Conceptual_metaphor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Conceptual Metaphor Theory&lt;/a&gt; hypothesizes that, through metaphor, humans creatively construe more abstract concepts in terms of more concrete concepts. In other words, language users imagine concepts that are hard to simulate (career hopes) in terms of those that are easier to simulate. Construing career hopes as a physical object makes sense, since their ‚Äúbreaking‚Äù evokes the notion that they may be hard to recover.&lt;/p&gt;
&lt;p&gt;Importantly, both mental simulation and metaphoric interpretation derive from interaction in the world rather than via static text. Accordingly, the paper argues for designing cognitively-inspired architectures, including diverse simulation environments through which the next generation of AI agents can begin learning the world knowledge necessary for more human-like language understanding.
Ultimately, the deeply embodied nature of language implies that we needn‚Äôt worry about AI poets surpassing humans in writing skill anytime soon. Rather, it suggests that a tighter integration between contemporary cognitive science and NLP research is a promising approach towards AI assistants with a better understanding of human language.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Talk at Tel Aviv University NLP Seminar</title>
      <link>https://ronentk.github.io/talk/tau_seminar_0420/</link>
      <pubDate>Thu, 30 Apr 2020 13:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/talk/tau_seminar_0420/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Language (Re)modelling: Towards Embodied Language Understanding</title>
      <link>https://ronentk.github.io/publication/tamari-2020-remodelling/</link>
      <pubDate>Mon, 13 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/publication/tamari-2020-remodelling/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Talk at Bar Ilan University NLP Seminar</title>
      <link>https://ronentk.github.io/talk/biu_seminar_0320/</link>
      <pubDate>Sun, 29 Mar 2020 11:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/talk/biu_seminar_0320/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Ecological Semantics: Programming Environments for Situated Language Understanding</title>
      <link>https://ronentk.github.io/publication/tamari-2020-ecological/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/publication/tamari-2020-ecological/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Popular science piece on embodied cognition and AI (Hebrew).</title>
      <link>https://ronentk.github.io/post/bac/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/post/bac/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.bac.org.il/society/article/bgvph-anhnv-mbynym&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;External link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Poster at AI Week Israel 2019</title>
      <link>https://ronentk.github.io/talk/ai_week_1119/</link>
      <pubDate>Mon, 18 Nov 2019 15:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/talk/ai_week_1119/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Y‚Ä≤all should read this! Identifying Plurality in Second-Person Personal Pronouns in English Texts</title>
      <link>https://ronentk.github.io/publication/stanovsky-tamari-2019-yall/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/publication/stanovsky-tamari-2019-yall/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Playing by the Book: An Interactive Game Approach for Action Graph Extraction from Text</title>
      <link>https://ronentk.github.io/publication/tamari-2018/</link>
      <pubDate>Sun, 02 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/publication/tamari-2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Talk at Tel Aviv Data Science Meetup</title>
      <link>https://ronentk.github.io/talk/ds_meetup_0419/</link>
      <pubDate>Fri, 19 Apr 2019 18:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/talk/ds_meetup_0419/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Talk at JerusML Meetup</title>
      <link>https://ronentk.github.io/talk/jerusml_meetup_0119/</link>
      <pubDate>Thu, 03 Jan 2019 20:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/talk/jerusml_meetup_0119/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Fair Consensus Protocol for Transaction Ordering</title>
      <link>https://ronentk.github.io/publication/asayag-2018/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/publication/asayag-2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Boosting Dilated Convolutional Networks with Mixed Tensor Decompositions</title>
      <link>https://ronentk.github.io/publication/cohen-2018-boosting/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/publication/cohen-2018-boosting/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Tensorial Mixture Models</title>
      <link>https://ronentk.github.io/publication/sharir-2016-a/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://ronentk.github.io/publication/sharir-2016-a/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
