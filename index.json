[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a PhD candidate in the Hyadata Data Science Lab in the Hebrew University of Jerusalem, under the supervision of Dafna Shahaf (HUJI) and Reut Tsarfaty (Bar Ilan University/Allen Institute for AI). Interested in how human intelligence can inform artificial intelligence, and vice versa. In particular, I\u0026rsquo;m interested in embodied/grounded cognition research and how it may help in building deeper language understanding and higher level cognition into AI systems.\nDuring my PhD, I\u0026rsquo;ve also been fortunate to have 3 wonderful summer internships: 2 at the Allen Institute for Artificial Intelligence in Seattle (mentored by Gabi Stanovsky and Kyle Richardson), and one with the RIKEN institute in Japan (under Prof. Yuji Matsumoto).\nOutside of the lab, I\u0026rsquo;m into various other embodied pursuits like trekking, cycling, yoga and world music. I\u0026rsquo;m passionate about socio-technical activism (e.g., RadicalXChange) for building more useful and equitable technology towards broader social change. I also do research at DAOStack on next-generation collaboration platforms for creating and harnessing collective intelligence.\nFor some random musings on the meaning of life, the universe and everything, check out my blog (Hebrew).\nüóûÔ∏è News\n September 2021: starting visiting research program with Prof. Judith Fan\u0026rsquo;s Cognitive Tools computational cognitive science/psychology lab at UCSD! June 2021: started a summer internship at AI2 with Kyle Richardson on the Aristo team.  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://ronentk.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m a PhD candidate in the Hyadata Data Science Lab in the Hebrew University of Jerusalem, under the supervision of Dafna Shahaf (HUJI) and Reut Tsarfaty (Bar Ilan University/Allen Institute for AI).","tags":null,"title":"","type":"authors"},{"authors":["Haoliang Wang","Jane Yang","Ronen Tamari","Judy Fan"],"categories":[""],"content":"","date":1655251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655251200,"objectID":"98c8379b36d04c419dddeed29ba3676e","permalink":"https://ronentk.github.io/publication/cliphy-2022/","publishdate":"2022-06-15T09:10:08.014852Z","relpermalink":"/publication/cliphy-2022/","section":"publication","summary":"The web has become a dominant epistemic environment, influencing people's beliefs at a global scale. However, online epistemic environments are increasingly polluted, impairing societies' ability to coordinate effectively in the face of global crises. We argue that centralized platforms are a main source of epistemic pollution, and that healthier environments require redesigning how we collectively govern attention. Inspired by decentralization and open source software movements, we propose Open Source Attention, a socio-technical framework for 'freeing' human attention from control by platforms, through a decentralized eco-system for creating, storing and querying stigmergic markers; the digital traces of human attention.","tags":null,"title":"Communicating understanding of physical dynamics in natural language","type":"publication"},{"authors":["Ronen Tamari","Daniel Friedman","William Fischer","Lauren Hebert","Dafna Shahaf"],"categories":["featured"],"content":"","date":1652572800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652572800,"objectID":"dbdbbb32e5e3eadb5c165f192b509a0d","permalink":"https://ronentk.github.io/publication/users-to-makers-2022/","publishdate":"2022-05-15T09:10:08.014852Z","relpermalink":"/publication/users-to-makers-2022/","section":"publication","summary":"The web has become a dominant epistemic environment, influencing people's beliefs at a global scale. However, online epistemic environments are increasingly polluted, impairing societies' ability to coordinate effectively in the face of global crises. We argue that centralized platforms are a main source of epistemic pollution, and that healthier environments require redesigning how we collectively govern attention. Inspired by decentralization and open source software movements, we propose Open Source Attention, a socio-technical framework for 'freeing' human attention from control by platforms, through a decentralized eco-system for creating, storing and querying stigmergic markers; the digital traces of human attention.","tags":null,"title":"From Users to (Sense)Makers: On the Pivotal Role of Stigmergic Social Annotation in the Quest for Collective Sensemaking","type":"publication"},{"authors":["Ronen Tamari","Kyle Richardson","Aviad Sar-Shalom","Noam Kahlon","Nelson Liu","Reut Tsarfaty","Dafna Shahaf"],"categories":["featured"],"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"5790e8368813b114aaef667fbd73731d","permalink":"https://ronentk.github.io/publication/dyna-babi-2021/","publishdate":"2022-05-01T09:10:08.014852Z","relpermalink":"/publication/dyna-babi-2021/","section":"publication","summary":"While neural language models often perform surprisingly well on natural language understanding (NLU) tasks, their strengths and limitations remain poorly understood. Controlled synthetic tasks are thus an increasingly important resource for diagnosing model behavior. In this work we focus on story understanding, a core competency for NLU systems. However, the main synthetic resource for story understanding, the bAbI benchmark, lacks such a systematic mechanism for controllable task generation. We develop Dyna-bAbI, a dynamic framework providing fine-grained control over task generation in bAbI. We demonstrate our ideas by constructing three new tasks requiring compositional generalization, an important evaluation setting absent from the original benchmark. We tested both special-purpose models developed for bAbI as well as state-of-the-art pre-trained methods, and found that while both approaches solve the original tasks (99% accuracy), neither approach succeeded in the compositional generalization setting, indicating the limitations of the original training data. We explored ways to augment the original data, and found that though diversifying training data was far more useful than simply increasing dataset size, it was still insufficient for driving robust compositional generalization (with ","tags":null,"title":"Dyna-bAbI: unlocking bAbI's potential with dynamic synthetic benchmarking","type":"publication"},{"authors":["Tom Hope","Ronen Tamari","Hyeonsu Kang","Daniel Hershcovich","Joel Chan","Aniket Kittur","Dafna Shahaf"],"categories":["featured"],"content":"","date":1643673600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643673600,"objectID":"50ae6f4133252e1aea3d880224c2d27b","permalink":"https://ronentk.github.io/publication/scaling-creation-2021/","publishdate":"2022-02-01T09:10:08.014852Z","relpermalink":"/publication/scaling-creation-2021/","section":"publication","summary":"Large repositories of products, patents and scientific papers offer an opportunity for building systems that scour millions of ideas and help users discover inspirations. However, idea descriptions are typically in the form of unstructured text, lacking key structure that is required for supporting creative innovation interactions. Prior work has explored idea representations that were either limited in expressivity, required significant manual effort from users, or dependent on curated knowledge bases with poor coverage. We explore a novel representation that automatically breaks up products into fine-grained functional aspects capturing the purposes and mechanisms of ideas, and use it to support important creative innovation interactions: functional search for ideas, and exploration of the design space around a focal problem by viewing related problem perspectives pooled from across many products. In user studies, our approach boosts the quality of creative search and inspirations, substantially outperforming strong baselines by 50-60%.","tags":null,"title":"Scaling Creative Inspiration with Fine-Grained Functional Aspects of Ideas","type":"publication"},{"authors":[],"categories":null,"content":"","date":1620172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620172800,"objectID":"671a0d7d9090887b6e45cb00f6bc43b5","permalink":"https://ronentk.github.io/talk/huji_seminar_0521/","publishdate":"2021-05-05T00:00:00Z","relpermalink":"/talk/huji_seminar_0521/","section":"talk","summary":"'Dark research' refers to skills and knowledge which are often significant factors in successful research, yet are rarely covered by any formal graduate university curricula. For example, (1) how to find and formulate research questions, (2) how to manage references \u0026 project workflows and not drown in the oceans of information, (3) how to write and maintain reproducible research code and experiments, and (4) how to forge connections and collaborations with the wider scientific community. In this meeting I'll present some of my experience and tools for grappling with these challenges, centering around Notion for (2) (and pretty much everything else also üôÉ), Weights and Biases for (3) and Twitter + Hugo Academic web pages for (4).","tags":[],"title":"'Dark Research' talk at Hebrew University NLP Seminar","type":"talk"},{"authors":null,"categories":null,"content":"\u0026ldquo;Dark research\u0026rdquo; refers to skills and knowledge which are often significant factors in successful research, yet are rarely covered by any formal graduate university curricula. For example, (1) how to find and formulate research questions, (2) how to manage references \u0026amp; project workflows and not drown in the oceans of information, (3) how to write and maintain reproducible research code and experiments, and (4) how to forge connections and collaborations with the wider scientific community. Here are some slides from a recent university seminar talk, presenting some of my own experience and tools for grappling with these challenges, centering around Notion for (2) (and pretty much everything else also üôÉ), Weights and Biases for (3) and Twitter + Hugo Academic web pages for (4).\n","date":1620172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620172800,"objectID":"3c20417290bf1d26c0c58fe60db2d664","permalink":"https://ronentk.github.io/post/dark_research/","publishdate":"2021-05-05T00:00:00Z","relpermalink":"/post/dark_research/","section":"post","summary":"\u0026ldquo;Dark research\u0026rdquo; refers to skills and knowledge which are often significant factors in successful research, yet are rarely covered by any formal graduate university curricula. For example, (1) how to find and formulate research questions, (2) how to manage references \u0026amp; project workflows and not drown in the oceans of information, (3) how to write and maintain reproducible research code and experiments, and (4) how to forge connections and collaborations with the wider scientific community.","tags":null,"title":"üîÆDark Research","type":"post"},{"authors":["Ronen Tamari","Fan Bai","Alan Ritter","Gabriel Stanovsky"],"categories":["featured"],"content":"","date":1618531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618531200,"objectID":"e8f291aaf5bedd2881f32ec7b3df38b8","permalink":"https://ronentk.github.io/publication/tamari-2020-process-eacl/","publishdate":"2021-01-27T09:10:08.014852Z","relpermalink":"/publication/tamari-2020-process-eacl/","section":"publication","summary":"We develop Process Execution Graphs (PEG), a document-level representation of real-world wet lab biochemistry protocols, addressing challenges such as cross-sentence relations, long-range coreference, grounding, and implicit arguments. We manually annotate PEGs in a corpus of complex lab protocols with a novel interactive textual simulator that keeps track of entity traits and semantic constraints during annotation. We use this data to develop graph-prediction models, finding them to be good at entity identification and local relation extraction, while our corpus facilitates further exploration of challenging long-range relations.","tags":null,"title":"Process-Level Representation of Scientific Protocols with Interactive Annotation","type":"publication"},{"authors":[],"categories":null,"content":"","date":1615939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615939200,"objectID":"0032690303164662b75da926d38effbf","permalink":"https://ronentk.github.io/talk/ucsd_0321/","publishdate":"2021-03-17T00:00:00Z","relpermalink":"/talk/ucsd_0321/","section":"talk","summary":"","tags":[],"title":"Cognitive Tools Lab, University of California, San Diego","type":"talk"},{"authors":[],"categories":null,"content":"","date":1610409600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610409600,"objectID":"1deca0a0b085fe437a0516c3b59ad0f6","permalink":"https://ronentk.github.io/talk/aristo_0121/","publishdate":"2021-01-12T00:00:00Z","relpermalink":"/talk/aristo_0121/","section":"talk","summary":"Presented our upcoming EACL 2021 paper paper.","tags":[],"title":"Seminar Talk at AI2 - Aristo","type":"talk"},{"authors":[],"categories":null,"content":"","date":1607472000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607472000,"objectID":"86bd2b4985cf8f40afd9674569f27539","permalink":"https://ronentk.github.io/talk/wordplay_2020/","publishdate":"2020-12-09T00:00:00Z","relpermalink":"/talk/wordplay_2020/","section":"talk","summary":"Presented our workshop paper.","tags":[],"title":"Contributed Talk (WordPlay @ NeurIPS2020)","type":"talk"},{"authors":null,"categories":null,"content":"External link\n","date":1607212800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607212800,"objectID":"0529c146d053e051b08a56a9fa0e034f","permalink":"https://ronentk.github.io/post/notion/","publishdate":"2020-12-06T00:00:00Z","relpermalink":"/post/notion/","section":"post","summary":"External link","tags":null,"title":"üîç Managing Research in Notion!","type":"post"},{"authors":["Ronen Tamari","Fan Bai","Alan Ritter","Gabriel Stanovsky"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"3ae5392585b361f2a385ea27e9d1839c","permalink":"https://ronentk.github.io/publication/tamari-2020-process-wp/","publishdate":"2020-03-15T09:10:08.014852Z","relpermalink":"/publication/tamari-2020-process-wp/","section":"publication","summary":"We develop Process Execution Graphs (PEG), an executable  document-level representation of real-world wet lab biochemistry protocols, addressing challenges such as cross-sentence relations, long-range coreference, grounding, and implicit arguments. We built a corpus of complex lab protocols  with  a novel interactive simulator built upon a text-based game engine that keeps track of entity traits and semantic constraints during annotation, yielding high quality annotated PEGs. Our framework presents several directions for future work, including the modelling of challenging long range dependencies, application of text-based games for real-world procedural text understanding, and extending simulation-based annotation to new domains.","tags":null,"title":"Process-Level Representation of Scientific Protocols with a Text-Based Game Annotation Interface","type":"publication"},{"authors":[],"categories":null,"content":"","date":1604926800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604926800,"objectID":"f0d034ae3f664764945aeb70e3800c22","permalink":"https://ronentk.github.io/talk/naisys_1120/","publishdate":"2020-11-09T13:00:00Z","relpermalink":"/talk/naisys_1120/","section":"talk","summary":"Presented poster of our ACL2020 paper.","tags":[],"title":"Poster at NAISys Virtual Conference","type":"talk"},{"authors":null,"categories":null,"content":"Background: Compositional Generalizations Compositional generalization, or models' lack of it, is a hot topic in NLP right now. Compositional generalization (Fodor \u0026amp; Pylysyn, 1988) refers to the ability to make \u0026ldquo;infinite use of finite means\u0026rdquo; and is a cornerstone of human intelligence in general, and in particular language use: humans can combine a finite set of discrete elements (such as words) in limitless ways to create new meanings. An oft-quoted example is that once humans learn the meaning of a new verb like \u0026ldquo;dax\u0026rdquo;, we effortlessly generalize to new novel combinations with known words, like \u0026ldquo;dax twice\u0026rdquo; or \u0026ldquo;dax slowly\u0026rdquo;. While crucial for more robust and efficient NLP, compositional generalization remains extremely challenging for state-of-the-art (SOTA) models, with a recent paper noting that despite the huge investment,\n General ML architecture improvements yield incremental, if limited, improvements in compositional generalization settings.\n How to even measure the ability for compositional generalization? Like so many concepts in natural language, intuitive explanations of compositional generalization are easy, but more precise definitions can be notoriously elusive. Many works (e.g., SCAN, gSCAN) adopt a simple approach of holding out some set of test samples which differ systematically from those seen in training. For example, showing a model what it means to \u0026ldquo;walk while spinning\u0026rdquo; and \u0026ldquo;push the circle\u0026rdquo; at train time, and then testing it on \u0026ldquo;push the circle while spinning\u0026rdquo;. While such approaches indeed test for compositional generalization, a limitation is that the holding out classes of samples is somewhat heuristic and hand-engineered, and doesn\u0026rsquo;t yield a precise, numeric or canonical quantification of train-test differences.\nFortunately, a cool paper presented this year at ICLR2020 makes some nice progress on this question. The paper, out of Google Brain, is called \u0026ldquo;Measuring Compositional Generalization: A Comprehensive Method on Realistic Data\u0026rdquo;. They propose a method, Distribution Based Compositionality Assessment (DBCA), which allows constructing and more precisely quantifying the compositional-generalization gap between train and test splits.\nI really liked the paper and indeed found it comprehensive (including a 20+ page appendix), so this is my attempt at a walk-through and accompaniment to my (un-offical) re-implementation.\nTurn the dial on the train-test compositionality gap, and watch model performance drop! If it doesn\u0026rsquo;t: either you have a breakthrough, or I have a bug üôÉ.\nDBCA in a nutshell DBCA provides a method of systematically generating datasets with train and test splits diverging in a controllable and measurable way. The paper applies this method to generate a compositional question answering dataset called CFQ (Compositional Freebase Questions). They show that turning the divergence dial strongly corresponds with a decrease in model accuracy, while remaining compositional and human interpretable; A human who could correctly answer the training set examples of ‚ÄúWho directed Inception?‚Äù and ‚ÄúDid Christopher Nolan produce Goldfinger?‚Äù would have no trouble testing on questions such as ‚ÄúDid Christopher Nolan direct Goldfinger?‚Äù and \u0026ldquo;Who produced Inception?\u0026rdquo;, but current SOTA models fail dramatically in such settings.\nAs the name implies, the magic happens by carefully controlling two distribution measures between train and test splits:\n  Atom Divergence: we\u0026rsquo;ll get formal later, but for now we can intuitively think of atoms as the elementary building blocks comprising each sample: predicates liked \u0026ldquo;directed\u0026rdquo;, or entities like \u0026ldquo;Inception\u0026rdquo;. We want the atoms to be distributed roughly the same across train and test distributions‚Üílow atom divergence, as shown in Fig. 6 of the paper\n  Compound Divergence: Compounds represent different ways of composing atoms. ‚ÄúDid Christopher Nolan direct Goldfinger?‚Äù and \u0026ldquo;Who produced Inception?\u0026rdquo; are novel compounds over the same atoms from the training set (\u0026ldquo;directed\u0026rdquo;, \u0026ldquo;Inception\u0026rdquo;, etc). To measure compositionality, we want to create test sets with novel compounds not seen at train time:\n  Definitions Sample Definition To meaningfully measure how samples differ from each other, let\u0026rsquo;s first understand how a sample is defined and generated in DBCA. Each sample is defined to be a directed a-cyclic graph (DAG), where nodes (or atoms) correspond to rule applications. Edges correspond to dependencies between rule applications.\nNodes: In CFQ, the paper mentions 4 kinds of rules used in question generation: grammar (211), inference (17), resolution (21) and knowledge (194). This yields a total of $N_{A}=443$ types of nodes, as can be seen in the histogram in Fig. 6 (above). Feel free to check out the paper for specific rule examples: this post is less about the particulars of CFQ and more the general workings of DBCA.\nEdges: DAG edges represent dependencies among the rules (nodes), an edge $A \\to B$ means that rule $B$ strictly depends on rule $A$ in the sense that the generator cannot apply rule $B$ before applying rule $A$.\nComparing graphs is known to be hard (graph isomorphism problem), therefore the DAG is normalized to ensure that a certain rule combination is represented using the same DAG across all the examples where it occurs. This is important for meaningfully comparing measures such as divergence.\nDistributions and Divergences Atom Distribution For a sample set $T$, the atom distribution, $\\mathcal{F}_{A}\\left(T\\right)$, is defined to be the frequency distribution of atoms in the set.\nHere $T$ is a collection of DAGs, and atoms correspond to graph node types. To be more concrete, you can imagine $\\mathcal{F}_{A} \\left( T \\right)$ being represented by a $N_{A}$ dimensional vector containing the normalized frequency counts of each atom (rule application) across $T$.\nAs mentioned, DBCA creates training and test sets which share similar atom distributions.\nCompound Distribution For a sample set $T$, the compound distribution, $\\mathcal{F}_{C}\\left(T\\right)$, is defined to be the frequency distribution of compounds in the set.\nIn contrast to the atom distribution, we want the compound distribution to be divergent across train and test sets, in order to test models' ability to recombine known atoms in novel ways. To represent $\\mathcal{F}_{C}\\left(T\\right)$, you can similarly imagine a long vector containing the frequency counts of each compound in $T$. Here we run into a problem though, due to combinatorial complexity considerations- the number of all possible compounds (sub-graphs) is generally exponential in the size of $T$.\nTherefore, for practical purposes, the compound distribution requires a more involved definition. Specifically, some reasoned method for selecting and counting the most interesting compounds must be employed. In CFQ, the chosen method was comprised of two steps:\n  Candidate sub-graph generation. From $T$, first generate some large sub-set of candidate sub-graphs, called $\\mathbb{G}$. The paper considered all sub-graphs with up to 5 nodes with branching factor up to 2, plus linear sub-graphs of all sizes (from personal correspondence with the authors).\n  Candidate ranking: As written in sec. 2.1, the frequency of many of these sub-graphs will be highly correlated with their super-graphs, leading to the undesirable possibility of double counting sub-graphs. Accordingly, a further weighting step is carried out, by which a weight is computed for each $G\\in\\mathbb{G}$. This weight takes into account each occurrence of $G$ in the sample set, so the final weight (un-normalized probability) of $G$ is given by $W\\left(G\\right)=\\sum_{R\\in T}w\\left(G,R\\right)$. The weight per sample $R$, $w\\left(G,R\\right)$, is calculated by:\n$$w\\left(G,R\\right)=\\max_{g\\in\\text{occ}\\left(G,R\\right)}\\left(1-\\max_{G':g\\prec g'\\in\\text{occ}\\left(G',R\\right)}P\\left(G'|G\\right)\\right)$$\nLet\u0026rsquo;s unpack what\u0026rsquo;s going on in this calculation (see also appendix L.4). $\\text{occ}\\left(G,R\\right)$ is the set of all occurrences of sub-graph $G$ in sample $R$ (one sub-graph may appear multiple times). The $\\prec$ denotes the strict sub-graph relation. $P\\left(G'|G\\right)$ is the empirical probability of $G'$ occurring as a super-graph of $G$ over the whole sample set $T$. So for each such occurrence $g$, we look at all super-graphs of $g$ in $R$, and take the super-graph $G'$ which co-occurs most often with $G$ over $T$.\nIntuitively, $w\\left(G,R\\right)$ estimates how interesting $G$ is in the context of $R$. The higher the probability of some super-graph $G'$ co-occurring with $G$ across $T$, the less interesting $G$ is. So for each occurrence $g$, we find its weight by taking the complement of this maximum empirical probability. To calculate the weight of $G$, we take the weight of the maximally interesting occurrence (the outer max).\nThis can be a little confusing, so let\u0026rsquo;s make it more concrete with a small example. Consider a toy sample set $T=\\left\\{ R_{1},R_{2},R_{3}\\right\\}$ and some sample sub-graphs $\\mathbb{G}=\\left\\{ G_{1},G_{2},\u0026hellip;\\right\\}$ as depicted below.\nFor this case, we would have $W\\left(G_{1},R_{1}\\right)=W\\left(G_{1},R_{2}\\right)=W\\left(G_{1},R_{3}\\right)=0$, since $G_1$ occurs only within the context of the super-graph $G_2$ across all samples. Conversely, $W\\left(G_{2},R_{1}\\right)=W\\left(G_{2},R_{2}\\right)=W\\left(G_{2},R_{3}\\right)=1$, since $G_2$ appears in a different context across $T$- there is no common super-graph of $G_2$ across all samples. These of course are extreme cases just to show the point of the weighting scheme, which\n ensures that when calculating compound divergence based on a weighted subset of compounds, the most representative compounds are taken into account, while avoiding double-counting compounds whose frequency of occurrence is already largely explainable by the frequency of occurrence of one of its super-compounds.\n Compound weight: Given the weights as defined above, we can calculate the un-normalized weight of a compound $G$ as $W\\left(G\\right)=\\sum_{R\\in T}W\\left(G,R\\right)$. We assume that $W\\left(G,R\\right)=0$ if $G$ doesn\u0026rsquo;t occur in $R$.\nIn practice, in CFQ the graphs in $\\mathbb{G}$ are then sorted according to descending $W$ and the top 100,000 are kept.\nProbability of a compound: To get the normalized probability, we can simply divide by the total sum over all of the weights for all compounds in the sample set:\n$$P\\left(G\\right)=\\frac{W\\left(G\\right)}{\\sum_{G'\\in\\mathbb{G}}W\\left(G'\\right)}$$\nDivergence Now we have all of the definitions in place to define distance (or divergence) between our train and test splits. DBCA uses the Chernoff coefficient which yields a scalar between 0 and 1, given two distributions $P$ and $Q$: $C_{\\alpha}\\left(P\\parallel Q\\right)=\\sum_{k}p_{k}^{\\alpha}q_{k}^{1-\\alpha}$. The more similar two distributions are, the closer this score will be to 1.\nWe\u0026rsquo;re measuring divergence though, so we take the complement to compute atom and compound divergences, as follows: $$\\mathcal{D}_{A}\\left(V\\parallel W\\right)=1 - C_{0.5}\\left(\\mathcal{F}_{A}\\left(V\\right)\\parallel\\mathcal{F}_{A}\\left(W\\right)\\right)$$\n$$\\mathcal{D}_{C}\\left(V\\parallel W\\right)=1 - C_{0.1}\\left(\\mathcal{F}_{C}\\left(V\\right)\\parallel\\mathcal{F}_{C}\\left(W\\right)\\right)$$\nWhere $V$ and $W$ are train and test sets, respectively.\nWhat is $\\alpha$ for? It serves to control the relative weight given to the train and test splits. As written in sec. 2.1 of the paper:\n For the atom divergence, we use Œ± = 0.5, which \u0026hellip; reflects the desire of making the atom distributions in train and test as similar as possible. For the compound divergence, we use Œ± = 0.1, which reflects the intuition that it is more important whether a certain compound occurs in P (train) than whether the probabilities in P (train) and Q (test) match exactly.\n Let\u0026rsquo;s make this more concrete with a toy example, assuming train and test distributions over just 4 compounds:\np = np.array([0.001, 0.001, 1-0.002, 0]) q = np.array([0.4,0.4,0,0.2]) C_alpha_1 = np.dot(p**0.1, q**0.9) # -\u0026gt; C_alpha_1 = 0.44 C_alpha_5 = np.dot(p**0.5, q**0.5) # -\u0026gt; C_alpha_5 = 0.04  So the mere existence in train set $P$ of even small counts of the first 2 compounds yields a high $C_{0.1}$-similarity with $Q$, where $C_{0.5}$ would only be high if the distributions were actually similar.\nArmed with these definitions, let\u0026rsquo;s see how they can be used in practice to generate train and test sets with desired atom and compound divergences.\n  Generating Splits The DBCA paper presents a pretty straightforward way to generate splits with some desired atom and compound divergences. The basic idea is to start with some pool of samples $U$, pre-compute the weights $W\\left(G,R\\right)$ for all considered sub-graphs, and then iteratively add a new sample to the train/test sets such that $\\mathcal{D}_{C}$ and $\\mathcal{D}_{A}$ are closest to the desired values $d_a$, $d_c$ (while also maintaining the train/test set size ratio). The paper doesn\u0026rsquo;t describe the greedy step explicitly, so in my implementation, in each iteration, I just add the sample minimizing the score function:\n$$S\\left(V,W\\right)=\\left|\\mathcal{D}_{C}\\left(V\\parallel W\\right)-d_{c}\\right|+\\left|\\mathcal{D}_{A}\\left(V\\parallel W\\right)-d_{a}\\right|$$\nAnother issue is that since this is a greedy algorithm, the paper notes that random restarts may be required by removing samples at certain iterations. There are no further details on this, and I haven\u0026rsquo;t yet incorporated it into my implementation.\nSo let\u0026rsquo;s look at the basic pseudocode to get a feel for the flow.\nPseudocode Inputs:\n $U$: a pool of samples. $d_a$, $d_c$: Target atom and compound divergences (in [0,1]).  Algorithm:\n Initialize empty sets $V$ (train) and $W$ (test). For each of the chosen candidate sub-graph types $G\\in\\mathbb{G}$ and for each sample $u$ in $U$, calculate $W\\left(G,u\\right)$, and retain the top $N$ sub-graphs (in terms of $W\\left(G\\right)$). Select random sample from $U$ and add to $V$. // initialization For $i=1\u0026hellip;(T-1)$:   If $\\text{is-train-step}\\left(i\\right)$:\n $u^*=\\arg\\min_{u\\in U}\\left(S\\left(V^{(i-1)}\\cup\\left\\{ u^*\\right\\} ,W^{(i-1)}\\right)\\right)$ $V^{(i)}=V^{(i-1)}\\cup \\left\\{ u^* \\right\\}, W^{(i)}=W^{(i-1)} $  else: // test step\n $u^*=\\arg\\min_{u\\in U}\\left(S\\left(V^{(i-1)} ,W^{(i-1)}\\cup\\left\\{ u^* \\right\\}\\right)\\right)$ $W^{(i)}=W^{(i-1)}\\cup\\left\\{ u^*\\right\\},V^{(i)}=V^{(i-1)}$      Open Questions  DBCA is reliant on the ability to generate data in a controllable way, thus requiring some kind of simulator or grammar. An interesting question is how to incorporate it with approaches for more natural language, like crowdsourcing or NLG. The current approach addresses short question/answer pairs, I wonder how it would scale to larger and more complex graphs.  Repo Feel free to check out (and improve) my implementation at https://github.com/ronentk/dbca-splitter !\n","date":1603065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603065600,"objectID":"b52a2bfb8d09dcfa613f0506e8852bcc","permalink":"https://ronentk.github.io/post/dbca/","publishdate":"2020-10-19T00:00:00Z","relpermalink":"/post/dbca/","section":"post","summary":"Blog post write-up of DBCA paper (Keysers et. al, 2020)","tags":null,"title":"Distribution Based Compositionality Assessment (DBCA)","type":"post"},{"authors":null,"categories":null,"content":" \u0026ldquo;A SONG of the rolling earth, and of words according,\nWere you thinking that those were the words, those upright lines? those curves, angles, dots?\nNo, those are not the words, the substantial words are in the ground and sea,\nThey are in the air, they are in you.\u0026rdquo; \u0026ndash; Walt Whitman\n Natural language processing (NLP) has become one of AI‚Äôs hottest research areas. Deep learning algorithms drawing upon massive amounts of data and specialized hardware have achieved impressive empirical progress, powering applications from multilingual machine translation to wise-cracking chatbots. However, state-of-the-art NLP algorithms continue to struggle with tasks that schoolchildren find trivial. For example, consider the following story: ‚ÄúJohn dropped the wine glass on the coffee table and watched with dismay as it shattered into pieces.‚Äù\nMost people would easily identify ‚Äúit‚Äù as referring to the wine glass. However, current NLP algorithms find this deduction difficult; most algorithms we tested judged it likely that both the wine glass and the coffee table shattered into pieces. The situation becomes even more complex with sentences whose meaning is not literal, such as ‚ÄúJohn‚Äôs career hopes shattered.‚Äù\nPerhaps the problem is the way that these algorithms learn about the world. Typically, NLP algorithms read terabytes of texts, extracting statistical patterns of language use from them. However, research in the Cognitive Sciences, and specifically Embodied Cognitive Linguistics (ECL), argues that when people communicate, a lot of the meaning is not even present in the words. Instead, ECL claims that people use embodied world knowledge to understand language, both literal (wine glasses) and non-literal (career hopes). Embodied means such knowledge is deeply dependent on features of human physical bodies, such as the abilities of locomotion, perception, and emotion.\nLanguage (Re)modelling: Towards Embodied Language Understanding aims to integrate ideas from ECL into current NLP research. The work highlights two crucial cognitive capabilities missing in today‚Äôs NLP methods: mental simulation and metaphoric interpretation.\nAccording to ECL‚Äôs Simulation Hypothesis, language users come to the text armed with vast world knowledge and a powerful imagination that facilitates conjuring detailed simulations of the world to achieve language understanding, such as knowing what happens when a wine glass drops. To simulate a more abstract concept like ‚Äúcareer hopes‚Äù, ECL‚Äôs Conceptual Metaphor Theory hypothesizes that, through metaphor, humans creatively construe more abstract concepts in terms of more concrete concepts. In other words, language users imagine concepts that are hard to simulate (career hopes) in terms of those that are easier to simulate. Construing career hopes as a physical object makes sense, since their ‚Äúbreaking‚Äù evokes the notion that they may be hard to recover.\nImportantly, both mental simulation and metaphoric interpretation derive from interaction in the world rather than via static text. Accordingly, the paper argues for designing cognitively-inspired architectures, including diverse simulation environments through which the next generation of AI agents can begin learning the world knowledge necessary for more human-like language understanding. Ultimately, the deeply embodied nature of language implies that we needn‚Äôt worry about AI poets surpassing humans in writing skill anytime soon. Rather, it suggests that a tighter integration between contemporary cognitive science and NLP research is a promising approach towards AI assistants with a better understanding of human language.\n","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"f8329d62c7d833ba2148c0eebdbc9dc3","permalink":"https://ronentk.github.io/post/acl2020-blog/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/post/acl2020-blog/","section":"post","summary":"Blog version of our upcoming ACL2020 Theme Track paper","tags":null,"title":"Language (Re)modelling: Towards Embodied Language Understanding (blog version)","type":"post"},{"authors":[],"categories":null,"content":"","date":1588251600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588251600,"objectID":"5cb4fe93dfee5f142093462b5185d7b6","permalink":"https://ronentk.github.io/talk/tau_seminar_0420/","publishdate":"2020-04-30T13:00:00Z","relpermalink":"/talk/tau_seminar_0420/","section":"talk","summary":"Ground Truth To Grounded Truth: Contemporary Cognitive Science Perspectives on Semantics \u0026 NLU","tags":[],"title":"Talk at Tel Aviv University NLP Seminar","type":"talk"},{"authors":["Ronen Tamari","Chen Shani","Tom Hope","Miriam Petruck","Omri Abend","Dafna Shahaf"],"categories":["featured"],"content":"","date":1586736000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586736000,"objectID":"e12e714ee7e26ac897126a2cffb30a86","permalink":"https://ronentk.github.io/publication/tamari-2020-remodelling/","publishdate":"2020-04-13T09:10:08.014852Z","relpermalink":"/publication/tamari-2020-remodelling/","section":"publication","summary":"While natural language understanding (NLU) is advancing rapidly, today's technology differs from human-like language understanding in fundamental ways, notably in its inferior efficiency, interpretability, and generalization. This work proposes an approach to representation and learning based on the tenets of embodied cognitive linguistics (ECL). According to ECL, natural language is inherently executable (like programming languages), driven by mental simulation and metaphoric mappings over hierarchical compositions of structures and schemata learned through embodied interaction. This position paper argues that the use of grounding by metaphoric inference and simulation will greatly benefit NLU systems,  and proposes a system architecture along with a roadmap towards realizing this vision.","tags":null,"title":"Language (Re)modelling: Towards Embodied Language Understanding","type":"publication"},{"authors":[],"categories":null,"content":"","date":1585479600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585479600,"objectID":"d5a80e57b1563d83fa53bb11be9bc3d3","permalink":"https://ronentk.github.io/talk/biu_seminar_0320/","publishdate":"2020-03-29T11:00:00Z","relpermalink":"/talk/biu_seminar_0320/","section":"talk","summary":"Ground Truth To Grounded Truth: Contemporary Cognitive Science Perspectives on Semantics \u0026 NLU","tags":[],"title":"Talk at Bar Ilan University NLP Seminar","type":"talk"},{"authors":["Ronen Tamari","Gabriel Stanovsky","Dafna Shahaf","Reut Tsarfaty"],"categories":["featured"],"content":"","date":1583798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583798400,"objectID":"c02a5710ba43de7f6f6b407fcb53f42f","permalink":"https://ronentk.github.io/publication/tamari-2020-ecological/","publishdate":"2020-03-15T09:10:08.014852Z","relpermalink":"/publication/tamari-2020-ecological/","section":"publication","summary":"Large-scale natural language understanding (NLU) systems have made impressive progress: they can be applied flexibly across a variety of tasks, and employ minimal structural assumptions. However, extensive empirical research has shown this to be a double-edged sword, coming at the cost of shallow understanding: inferior generalization, grounding and explainability. Grounded language learning approaches offer the promise of deeper understanding by situating learning in richer, more structured training environments, but are limited in scale to relatively narrow, predefined domains. How might we enjoy the best of both worlds: grounded, general NLU? Following extensive contemporary cognitive science, we propose treating environments as ``first-class citizens'' in semantic representations, worthy of research and development in their own right. Importantly, models should also be partners in the creation and configuration of environments, rather than just actors within them, as in existing approaches. To do so, we argue that models must begin to understand and program in the language of affordances (which define possible actions in a given situation) both for online, situated discourse comprehension, as well as large-scale, offline common-sense knowledge mining. To this end we propose an environment-oriented ecological semantics, outlining theoretical and practical approaches towards implementation. We further provide actual demonstrations building upon interactive fiction programming languages.","tags":null,"title":"Ecological Semantics: Programming Environments for Situated Language Understanding","type":"publication"},{"authors":null,"categories":null,"content":"External link\n","date":1583798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583798400,"objectID":"94e7828830cb24774b914370b8c1da7b","permalink":"https://ronentk.github.io/post/bac/","publishdate":"2020-03-10T00:00:00Z","relpermalink":"/post/bac/","section":"post","summary":"External link","tags":null,"title":"Popular science piece on embodied cognition and AI (Hebrew).","type":"post"},{"authors":[],"categories":null,"content":"","date":1574089200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574089200,"objectID":"cb18f27ce6ba16485a9a72c1ba4560c6","permalink":"https://ronentk.github.io/talk/ai_week_1119/","publishdate":"2019-11-18T15:00:00Z","relpermalink":"/talk/ai_week_1119/","section":"talk","summary":"","tags":[],"title":"Poster at AI Week Israel 2019","type":"talk"},{"authors":["Gabriel Stanovsky","Ronen Tamari"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"f7d565a15f7d9a34781d70cc0b7c73d7","permalink":"https://ronentk.github.io/publication/stanovsky-tamari-2019-yall/","publishdate":"2020-03-15T09:01:55.258506Z","relpermalink":"/publication/stanovsky-tamari-2019-yall/","section":"publication","summary":"Distinguishing between singular and plural ``you‚Ä≥ in English is a challenging task which has potential for downstream applications, such as machine translation or coreference resolution. While formal written English does not distinguish between these cases, other languages (such as Spanish), as well as other dialects of English (via phrases such as ``y‚Ä≤all‚Ä≥), do make this distinction. We make use of this to obtain distantly-supervised labels for the task on a large-scale in two domains. Following, we train a model to distinguish between the single/plural `you‚Ä≤, finding that although in-domain training achieves reasonable accuracy (mbox$‚â•$ 77%), there is still a lot of room for improvement, especially in the domain-transfer scenario, which proves extremely challenging. Our code and data are publicly available.","tags":null,"title":"Y‚Ä≤all should read this! Identifying Plurality in Second-Person Personal Pronouns in English Texts","type":"publication"},{"authors":["Ronen Tamari","Hiroyuki Shindo","Dafna Shahaf","Yuji Matsumoto"],"categories":["featured"],"content":"","date":1559433600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559433600,"objectID":"e0615360424c6352ec19e504be8dc5cc","permalink":"https://ronentk.github.io/publication/tamari-2018/","publishdate":"2020-03-15T08:52:20.828604Z","relpermalink":"/publication/tamari-2018/","section":"publication","summary":"Understanding procedural text requires tracking entities, actions and effects as the narrative unfolds. We focus on the challenging real-world problem of action-graph extraction from material science papers, where language is highly specialized and data annotation is expensive and scarce. We propose a novel approach, Text2Quest, where procedural text is interpreted as instructions for an interactive game. A learning agent completes the game by executing the procedure correctly in a text-based simulated lab environment. The framework can complement existing approaches and enables richer forms of learning compared to static texts. We discuss potential limitations and advantages of the approach, and release a prototype proof-of-concept, hoping to encourage research in this direction.","tags":null,"title":"Playing by the Book: An Interactive Game Approach for Action Graph Extraction from Text","type":"publication"},{"authors":[],"categories":null,"content":"","date":1555696800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555696800,"objectID":"67367646c89692a9eae2b4ce4ebebf94","permalink":"https://ronentk.github.io/talk/ds_meetup_0419/","publishdate":"2019-04-19T18:00:00Z","relpermalink":"/talk/ds_meetup_0419/","section":"talk","summary":"","tags":[],"title":"Talk at Tel Aviv Data Science Meetup","type":"talk"},{"authors":[],"categories":null,"content":"","date":1546545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546545600,"objectID":"41aa21eae6a3e8e129b72cdd2c7fddbd","permalink":"https://ronentk.github.io/talk/jerusml_meetup_0119/","publishdate":"2019-01-03T20:00:00Z","relpermalink":"/talk/jerusml_meetup_0119/","section":"talk","summary":"","tags":[],"title":"Talk at JerusML Meetup","type":"talk"},{"authors":["A. Asayag","G. Cohen","I. Grayevsky","M. Leshkowitz","O. Rottenstreich","R. Tamari","D. Yakira"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"a14a3dcb8053d181b9e433c1d063126f","permalink":"https://ronentk.github.io/publication/asayag-2018/","publishdate":"2020-03-15T09:01:55.259233Z","relpermalink":"/publication/asayag-2018/","section":"publication","summary":"We present Helix, a blockchain-based consensus protocol for fair ordering of transactions among nodes in a distributed network. Helix advances in rounds, where in each round, the primary node (elected among the network nodes) proposes a potential block (a successive set of transactions). In order to be included in the blockchain, a block must pass validation by an elected committee of nodes. Helix nodes are presumed to have two primary preferences. They prefer to be elected as committee members. Additionally, because each transaction is associated with one of the network nodes, they prefer to prioritize their own transactions over those of others. In light of these individual preferences, our definition of fairness incorporates three key elements. First, the process of electing nodes to committees is random and unpredictable. Second, a correlated sampling scheme is used in order to guarantee random selection and ordering of pending transactions in blocks. Third, transactions are encrypted in order to hide their associations with their respective nodes and prevent censorship. Through the corresponding threshold decryption process we obtain an unpredictable and non-manipulable randomness beacon, which serves both the election process and the correlated sampling scheme. We define a quantitative measure of fairness in the protocol, prove theoretically that fairness manipulation in Helix is significantly limited, and present experiments evaluating fairness in practice.","tags":null,"title":"A Fair Consensus Protocol for Transaction Ordering","type":"publication"},{"authors":["Nadav Cohen","Ronen Tamari","Amnon Shashua"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"1fa13f112cd5da7341084c281aab3187","permalink":"https://ronentk.github.io/publication/cohen-2018-boosting/","publishdate":"2020-03-15T09:01:55.259002Z","relpermalink":"/publication/cohen-2018-boosting/","section":"publication","summary":"","tags":null,"title":"Boosting Dilated Convolutional Networks with Mixed Tensor Decompositions","type":"publication"},{"authors":["Or Sharir","Ronen Tamari","Nadav Cohen","Amnon Shashua"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"ecb424885ef30aed3b787b82518bd041","permalink":"https://ronentk.github.io/publication/sharir-2016-a/","publishdate":"2020-03-15T09:01:55.259492Z","relpermalink":"/publication/sharir-2016-a/","section":"publication","summary":"We introduce a generative model, we call Tensorial Mixture Models (TMMs) based on mixtures of basic component distributions over local structures (e.g. patches in an image) where the dependencies between the local-structures are represented by a \"priors tensor\" holding the prior probabilities of assigning a component distribution to each local-structure. In their general form, TMMs are intractable as the prior tensor is typically of exponential size. However, when the priors tensor is decomposed it gives rise to an arithmetic circuit which in turn transforms the TMM into a Convolutional Arithmetic Circuit (ConvAC). A ConvAC corresponds to a shallow (single hidden layer) network when the priors tensor is decomposed by a CP (sum of rank-1) approach and corresponds to a deep network when the decomposition follows the Hierarchical Tucker (HT) model. The ConvAC representation of a TMM possesses several attractive properties. First, the inference is tractable and is implemented by a forward pass through a deep network. Second, the architectural design of the model follows the deep networks community design, i.e., the structure of TMMs is determined by just two easily understood factors: size of pooling windows and number of channels. Finally, we demonstrate the effectiveness of our model when tackling the problem of classification with missing data, leveraging TMMs unique ability of tractable marginalization which leads to optimal classifiers regardless of the missingness distribution.","tags":null,"title":"Tensorial Mixture Models","type":"publication"}]